{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ANLP_Project_SeqToSQL.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "fb809b3677754bdcaf6aa222b5e904de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ff93b9f5a7574239a1b225e67802a46a",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_188f24d628464342afb078d56c8a70a3",
       "IPY_MODEL_2a9a368356f44ae9bfa392a3f6b8bc4a"
      ]
     }
    },
    "ff93b9f5a7574239a1b225e67802a46a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "188f24d628464342afb078d56c8a70a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_9d6f0e8ada0e4b539bd87a078f1dbef4",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 213450,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 213450,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_4b942ac9e99f4ff9844a5c5f02f5b520"
     }
    },
    "2a9a368356f44ae9bfa392a3f6b8bc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_cc5fc7560b684aa69ad61777002378e1",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 213k/213k [00:00&lt;00:00, 536kB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_06d9784d6962497580089de9a4af5a67"
     }
    },
    "9d6f0e8ada0e4b539bd87a078f1dbef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "4b942ac9e99f4ff9844a5c5f02f5b520": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "cc5fc7560b684aa69ad61777002378e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "06d9784d6962497580089de9a4af5a67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "07e05e4e2a694c31a2c3bcdb6d5586ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_46acbe4de8594572a276b86454a11602",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_5ac7d0f4bfc64e9e912516cd9894f4ea",
       "IPY_MODEL_ed186036d82743b4bbaeb7b421bb04d8"
      ]
     }
    },
    "46acbe4de8594572a276b86454a11602": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "5ac7d0f4bfc64e9e912516cd9894f4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_13ced24dc1e2472c966d6dbe91be01ab",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 433,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 433,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_86c01d2179d74763b5b957fc942001df"
     }
    },
    "ed186036d82743b4bbaeb7b421bb04d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_fa861b8e35ee440aaef9e38342ab410b",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 433/433 [00:00&lt;00:00, 998B/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_1f43877e3c0d42da8634d6200ab588b1"
     }
    },
    "13ced24dc1e2472c966d6dbe91be01ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "86c01d2179d74763b5b957fc942001df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "fa861b8e35ee440aaef9e38342ab410b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "1f43877e3c0d42da8634d6200ab588b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "246babcbaf074305bd4f888e12fd4b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_7fded0dab11f4fdc985c1c173b7097a3",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_b52e07a6ffd443da868dd2e9225a7eeb",
       "IPY_MODEL_5f4ac871552542e1adc08228c2252256"
      ]
     }
    },
    "7fded0dab11f4fdc985c1c173b7097a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "b52e07a6ffd443da868dd2e9225a7eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_ce72d94b9a7e461fbe45ab6a6ce982da",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 435779157,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 435779157,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2e1647e162c344d5914fd295d06a2914"
     }
    },
    "5f4ac871552542e1adc08228c2252256": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_1531bf8cc3ec4448ab1115b1da49f75f",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 436M/436M [00:08&lt;00:00, 53.0MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6ebf2f22e1f94960bf029a3a25ede4c7"
     }
    },
    "ce72d94b9a7e461fbe45ab6a6ce982da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2e1647e162c344d5914fd295d06a2914": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "1531bf8cc3ec4448ab1115b1da49f75f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "6ebf2f22e1f94960bf029a3a25ede4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73U6yPyZmhGL"
   },
   "source": [
    "# Sequence To SQL\n",
    "\n",
    "Welcome to our project in the Advanced Natural Language Processing course\n",
    "\n",
    "We try to build it with the data provided in https://github.com/salesforce/WikiSQL\n",
    "\n",
    "Remove this: https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?gi=6b6c7e91e298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPUHkkwhVukj",
    "outputId": "d365e360-6c44-4342-f1c7-c32ebe5c8bcc"
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find GPU. Will use CPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection and Review\n",
    "Clone the data from the WikiSQL git repository and install them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMz3n3dbHlU"
   },
   "source": [
    "Take a look inside the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zR6cyK-UbFNV"
   },
   "source": [
    "import json\n",
    "\n",
    "def read_json_data_from_file(file: str):\n",
    "  ret_data = []\n",
    "  with open(file) as json_file:\n",
    "      # Get next line from file\n",
    "      lines = json_file.readlines()\n",
    "      for line in tqdm(lines):\n",
    "          if not line:\n",
    "            break\n",
    "\n",
    "          data = json.loads(line)\n",
    "          ret_data.append(data)\n",
    "  return ret_data\n",
    "\n",
    "def convert_to_id_dict(data, id_key: str):\n",
    "  ret_dict = {}\n",
    "  for element in data:\n",
    "    if id_key in element:\n",
    "      ret_dict[element[id_key]] = element\n",
    "    else:\n",
    "      print(f'Element {element} doenst contain key {id_key}')\n",
    "  return ret_dict"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A80zkDsgqjE"
   },
   "source": [
    "Lets see if we succesfully serialized the data into objects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sihMUsXg-yt",
    "outputId": "8317c04e-70bf-41bf-8f8c-c4ba39c5fd30"
   },
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    " \n",
    "print(f'We have {len(dev_req_data)} dev data with {len(dev_table_data)} tables.')\n",
    "print(f'An example Request: ')\n",
    "print(json.dumps(dev_req_data[0], indent=2))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22a97f8ad87d48238a0bfb0eb71d6a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "879d10885c744c7d83cbb200f93f09a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8421 dev data with 2716 tables.\n",
      "An example Request: \n",
      "{\n",
      "  \"phase\": 1,\n",
      "  \"table_id\": \"1-10015132-11\",\n",
      "  \"question\": \"What position does the player who played for butler cc (ks) play?\",\n",
      "  \"sql\": {\n",
      "    \"sel\": 3,\n",
      "    \"conds\": [\n",
      "      [\n",
      "        5,\n",
      "        0,\n",
      "        \"Butler CC (KS)\"\n",
      "      ]\n",
      "    ],\n",
      "    \"agg\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzjw8292zYcE"
   },
   "source": [
    "### The fields represent the following:\n",
    "\n",
    "* `phase`: the phase in which the dataset was collected. We collected WikiSQL in two phases.\n",
    "* `question`: the natural language question written by the worker.\n",
    "* `table_id`: the ID of the table to which this question is addressed.\n",
    "sql: the SQL query corresponding to the question. This has the following *subfields:\n",
    "  * `sel`: the numerical index of the column that is being selected. You can find the actual column from the table.\n",
    "  * `agg`: the numerical index of the aggregation operator that is being used. You can find the actual operator from Query.agg_ops in lib/query.py.\n",
    "  * `conds`: a list of triplets (column_index, operator_index, condition) where:\n",
    "    * `column_index`: the numerical index of the condition column that is being used. You can find the actual column from the table.\n",
    "    * `operator_index`: the numerical index of the condition operator that is being used. You can find the actual operator from Query.cond_ops in lib/query.py.\n",
    "    * `condition`: the comparison value for the condition, in either string or float type.\n",
    "\n",
    "\n",
    "## An example Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f7vfdbCKGG3p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2fb59ef-25a3-4d4f-d652-5f8f0d6b796d"
   },
   "source": [
    "print(json.dumps(dev_table_data[0], indent=2))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"header\": [\n",
      "    \"Player\",\n",
      "    \"No.\",\n",
      "    \"Nationality\",\n",
      "    \"Position\",\n",
      "    \"Years in Toronto\",\n",
      "    \"School/Club Team\"\n",
      "  ],\n",
      "  \"page_title\": \"Toronto Raptors all-time roster\",\n",
      "  \"types\": [\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"id\": \"1-10015132-11\",\n",
      "  \"section_title\": \"L\",\n",
      "  \"caption\": \"L\",\n",
      "  \"rows\": [\n",
      "    [\n",
      "      \"Antonio Lang\",\n",
      "      \"21\",\n",
      "      \"United States\",\n",
      "      \"Guard-Forward\",\n",
      "      \"1999-2000\",\n",
      "      \"Duke\"\n",
      "    ],\n",
      "    [\n",
      "      \"Voshon Lenard\",\n",
      "      \"2\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"2002-03\",\n",
      "      \"Minnesota\"\n",
      "    ],\n",
      "    [\n",
      "      \"Martin Lewis\",\n",
      "      \"32, 44\",\n",
      "      \"United States\",\n",
      "      \"Guard-Forward\",\n",
      "      \"1996-97\",\n",
      "      \"Butler CC (KS)\"\n",
      "    ],\n",
      "    [\n",
      "      \"Brad Lohaus\",\n",
      "      \"33\",\n",
      "      \"United States\",\n",
      "      \"Forward-Center\",\n",
      "      \"1996\",\n",
      "      \"Iowa\"\n",
      "    ],\n",
      "    [\n",
      "      \"Art Long\",\n",
      "      \"42\",\n",
      "      \"United States\",\n",
      "      \"Forward-Center\",\n",
      "      \"2002-03\",\n",
      "      \"Cincinnati\"\n",
      "    ],\n",
      "    [\n",
      "      \"John Long\",\n",
      "      \"25\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"1996-97\",\n",
      "      \"Detroit\"\n",
      "    ],\n",
      "    [\n",
      "      \"Kyle Lowry\",\n",
      "      \"3\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"2012-Present\",\n",
      "      \"Villanova\"\n",
      "    ]\n",
      "  ],\n",
      "  \"name\": \"table_10015132_11\"\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7i0WqeKz2Cy"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "The data is stored with indices but we need the actual column names so saturate the requests with the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gwsK8yDV2_yD"
   },
   "source": [
    "# Transform the data into a dictonary index by the id\n",
    "dev_table_data_dict = convert_to_id_dict(dev_table_data, 'id')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iIAcsIpd7OpY"
   },
   "source": [
    "# Get the preliminary data\n",
    "# Maybe we want the other idexes also"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rnvuug6C0kaK"
   },
   "source": [
    "def get_table_column(data_list, tables_dict):\n",
    "  ret_list = []\n",
    "  for element in data_list:\n",
    "    current_table = tables_dict[element['table_id']]\n",
    "    columns = current_table['header']\n",
    "    # Replace the index\n",
    "    element['columns'] = columns\n",
    "    element['types'] = current_table['types']\n",
    "    element['sql']['sel_name'] = columns[element['sql']['sel']]\n",
    "\n",
    "    if 'page_title' in current_table:\n",
    "        element['table_name'] = current_table['page_title']\n",
    "    elif 'section_title' in current_table:\n",
    "        element['table_name'] = current_table['section_title']\n",
    "    elif 'caption' in current_table:\n",
    "        element['table_name'] = current_table['caption']\n",
    "    elif 'name' in current_table:\n",
    "        element['table_name'] = current_table['name']\n",
    "\n",
    "    # For the where conditions\n",
    "    for cond in element['sql']['conds']:\n",
    "      cond[0] = columns[cond[0]]\n",
    "    ret_list.append(element)\n",
    "  return ret_list\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cv1VKePBUwf",
    "outputId": "5f52c637-26e3-47b1-c111-cd10b31d1494"
   },
   "source": [
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "dev_prep_req_data = get_table_column(dev_req_data, dev_table_data_dict)\n",
    "\n",
    "\n",
    "print(f'Filed in with the Columns: ')\n",
    "print(json.dumps(dev_prep_req_data[-2], indent=2))\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3133775fda94a439a7a91c260d0e839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b604c0724f4141f49a3422bad1a94b13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filed in with the Columns: \n",
      "{\n",
      "  \"phase\": 2,\n",
      "  \"table_id\": \"2-12601141-1\",\n",
      "  \"question\": \"Which state does Jimmy Quillen represent?\",\n",
      "  \"sql\": {\n",
      "    \"sel\": 2,\n",
      "    \"conds\": [\n",
      "      [\n",
      "        \"Representative\",\n",
      "        0,\n",
      "        \"jimmy quillen\"\n",
      "      ]\n",
      "    ],\n",
      "    \"agg\": 0,\n",
      "    \"sel_name\": \"State\"\n",
      "  },\n",
      "  \"columns\": [\n",
      "    \"Representative\",\n",
      "    \"Years\",\n",
      "    \"State\",\n",
      "    \"Party\",\n",
      "    \"Lifespan\"\n",
      "  ],\n",
      "  \"types\": [\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"table_name\": \"List of former members of the United States House of Representatives (Q)\"\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# TODO figure out a good padding size or how to do padding correctly\n",
    "\n",
    "def get_question_answers(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    max_len = 0\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        if max_len < len(embedding['input_ids']):\n",
    "            max_len = len(embedding['input_ids'])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "pad_max_length = 50\n",
    "def get_question_answers_def_length(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create data loader that transformed the stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Task(Enum):\n",
    "    SELECT = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "733e8ac5e6a94795857c4bec2b354478"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e42ff014ea7454a92632905b1f82da6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8421 dev data with 2716 tables.\n",
      "dict_keys(['request', 'input_ids', 'token_type_ids', 'attention_mask', 'target'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'request': {'phase': tensor([1]),\n  'table_id': ['1-10015132-11'],\n  'question': ['What position does the player who played for butler cc (ks) play?'],\n  'sql': {'sel': tensor([3]),\n   'conds': [[('School/Club Team',), tensor([0]), ('Butler CC (KS)',)]],\n   'agg': tensor([0]),\n   'sel_name': ['Position']},\n  'columns': [('Player',),\n   ('No.',),\n   ('Nationality',),\n   ('Position',),\n   ('Years in Toronto',),\n   ('School/Club Team',)],\n  'types': [('text',), ('text',), ('text',), ('text',), ('text',), ('text',)],\n  'table_name': ['Toronto Raptors all-time roster']},\n 'input_ids': tensor([[[  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197,  5348,\n             102,  1327,  1700,  1674,  1103,  1591,  1150,  1307,  1111, 23635,\n           14402,   113,   180,  1116,   114,  1505,   136,   102,     0,     0,\n               0],\n          [  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197,  1302,\n             119,   102,  1327,  1700,  1674,  1103,  1591,  1150,  1307,  1111,\n           23635, 14402,   113,   180,  1116,   114,  1505,   136,   102,     0,\n               0],\n          [  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197,  1305,\n            1785,   102,  1327,  1700,  1674,  1103,  1591,  1150,  1307,  1111,\n           23635, 14402,   113,   180,  1116,   114,  1505,   136,   102,     0,\n               0],\n          [  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197, 18959,\n            5053,  2116,   102,  1327,  1700,  1674,  1103,  1591,  1150,  1307,\n            1111, 23635, 14402,   113,   180,  1116,   114,  1505,   136,   102,\n               0],\n          [  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197,  5848,\n            1107,  3506,   102,  1327,  1700,  1674,  1103,  1591,  1150,  1307,\n            1111, 23635, 14402,   113,   180,  1116,   114,  1505,   136,   102,\n               0],\n          [  101,  3087,  3506, 21196,  5067,  1155,   118,  1159,  9197,  1323,\n             120,  1998,  2649,   102,  1327,  1700,  1674,  1103,  1591,  1150,\n            1307,  1111, 23635, 14402,   113,   180,  1116,   114,  1505,   136,\n             102]]]),\n 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 0, 0, 0],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 0, 0],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 0, 0],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 0],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 0],\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1]]]),\n 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1]]]),\n 'target': tensor([[3]])}"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WikiSQLDataset(Dataset):\n",
    "\n",
    "    def __init__(self, requests, tokenizer, task: Task):\n",
    "        self.requests = requests\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.requests)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        req = self.requests[item]\n",
    "        _req_embeddings = get_question_answers(req, self.tokenizer)\n",
    "        _input_ids = [req_embedding['input_ids'] for req_embedding in _req_embeddings]\n",
    "        _token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _req_embeddings]\n",
    "        _attention_mask = [req_embedding['attention_mask'] for req_embedding in _req_embeddings]\n",
    "\n",
    "        target = None\n",
    "        if self.task == Task.SELECT:\n",
    "            correct_sel_id = req['sql']['sel']\n",
    "            target = torch.tensor([correct_sel_id], dtype=torch.long)\n",
    "\n",
    "        return dict(\n",
    "            request = self.requests[item],\n",
    "            input_ids = torch.tensor(_input_ids),\n",
    "            token_type_ids = torch.tensor(_token_type_ids),\n",
    "            attention_mask = torch.tensor(_attention_mask),\n",
    "            target = target\n",
    "        )\n",
    "\n",
    "def get_data_loader(data_type, tokenizer, task, batch_size):\n",
    "    # TODO check if we can use DataLoader with batch size as done in the tutorial\n",
    "    loaded_req = read_json_data_from_file(f'{data_folder}/{data_type}.jsonl')\n",
    "    loaded_tables = read_json_data_from_file(f'{data_folder}/{data_type}.tables.jsonl')\n",
    "    table_data_dict = convert_to_id_dict(loaded_tables, 'id')\n",
    "\n",
    "    prep_req_data = get_table_column(loaded_req, table_data_dict)\n",
    "\n",
    "    print(f'We have {len(loaded_req)} {data_type} data with {len(loaded_tables)} tables.')\n",
    "\n",
    "    return DataLoader(\n",
    "        WikiSQLDataset(requests = prep_req_data, tokenizer = tokenizer, task=task),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "data_folder = '../data'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "\n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnTnkOXH66r"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ndN0Emuk2K-"
   },
   "source": [
    "In this section, we will look into **contextual embeddings**. \n",
    "\n",
    "For this we use [**pretrained BERT**](https://www.aclweb.org/anthology/N19-1423.pdf) provided via [HuggingFace](https://huggingface.co/).\n",
    "\n",
    "Let's first install the HuggingFace python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Dp imports here\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyLYVYNrC7Pl",
    "outputId": "827654c5-31a4-4e57-c9ef-4760e5cf20eb"
   },
   "source": [
    "print(dev_prep_req_data[0])\n",
    "for tokens in get_question_answers(dev_prep_req_data[0], tokenizer):\n",
    "    print(tokens)\n",
    "    print(tokenizer.convert_ids_to_tokens(tokens['input_ids']))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phase': 1, 'table_id': '1-10015132-11', 'question': 'What position does the player who played for butler cc (ks) play?', 'sql': {'sel': 3, 'conds': [['School/Club Team', 0, 'Butler CC (KS)']], 'agg': 0, 'sel_name': 'Position'}, 'columns': ['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team'], 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'table_name': 'Toronto Raptors all-time roster'}\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -3, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 5348, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Player', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -2, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1302, 119, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'No', '.', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -2, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1305, 1785, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'National', '##ity', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 18959, 5053, 2116, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Po', '##si', '##tion', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 5848, 1107, 3506, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Years', 'in', 'Toronto', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': 0, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1323, 120, 1998, 2649, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'School', '/', 'Club', 'Team', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Generall model configuration\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zeujv-VJIHGy"
   },
   "source": [
    "# reference is hydranet https://arxiv.org/pdf/2008.04759.pdf\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 7725, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 5848, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1426, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1786, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': 0, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 2583, 27894, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "req_embeddings = get_question_answers(dev_prep_req_data[-2], tokenizer)\n",
    "print(req_embeddings)\n",
    "input_ids = [req_embedding['input_ids'] for req_embedding in req_embeddings]\n",
    "token_type_ids = [req_embedding['token_type_ids'] for req_embedding in req_embeddings]\n",
    "attention_mask = [req_embedding['attention_mask'] for req_embedding in req_embeddings]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 7725, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 5848, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1426, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1786, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 2583, 27894, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n"
     ]
    },
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3460, -0.0871, -0.2241,  ...,  0.1654,  0.0933,  0.1207],\n         [ 0.2659, -0.2906, -0.4186,  ...,  0.4836,  0.4394,  0.0897],\n         [ 0.2941, -0.3711, -0.5376,  ...,  0.2233,  0.6564,  0.2380],\n         ...,\n         [ 0.4445, -0.6749, -0.5239,  ...,  0.1327, -0.2456, -0.4662],\n         [ 0.3296, -0.3824, -0.5856,  ...,  0.4956, -0.0215,  0.0984],\n         [ 0.1314, -0.2563, -0.6348,  ...,  0.8775,  0.6959,  0.1449]]],\n       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5832,  0.5481,  0.9997,  ...,  0.9999, -0.4037,  0.9621]],\n       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_tensor = torch.tensor(input_ids)\n",
    "token_type_ids_tensor = torch.tensor(token_type_ids)\n",
    "attention_mask_tensor = torch.tensor(attention_mask)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids_tensor, # The tokens representing our input text.\n",
    "    attention_mask=token_type_ids_tensor,\n",
    "    token_type_ids=attention_mask_tensor\n",
    ") # The segment IDs to differentiate question from answer_text\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state  : torch.Size([5, 29, 768])\n",
      "pooled output layer: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f'last hidden state  : {outputs.last_hidden_state.shape}')\n",
    "print(f'pooled output layer: {outputs.pooler_output.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# weight = torch.rand(model.config.hidden_size)\n",
    "# F.sigmoid(F.linear(outputs.pooler_output, weight))\n",
    "\n",
    "out = nn.Linear(model.config.hidden_size, 1)\n",
    "out2 = torch.softmax(torch.sigmoid(out(outputs.pooler_output)), dim = 0)\n",
    "\n",
    "torch.argmax(out2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(dev_prep_req_data) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/56355 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95ee1863b5bc4e34aa290aa45019f4c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/18585 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "584dff7199af4004b8b4204d06da977d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 56355 train data with 18585 tables.\n",
      "We have 8421 dev data with 2716 tables.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d6d0d6d36d54b7585aa0a546781b535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf51dc9e460a451185486060cca5df99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_loader = get_data_loader(data_type='train', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "val_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "# test_data_loader = get_data_loader('train', tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class SelectionRanker(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SelectionRanker, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.squeeze(0),\n",
    "            attention_mask=attention_mask.squeeze(0),\n",
    "            token_type_ids=token_type_ids.squeeze(0)\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output)\n",
    "        softmax = torch.softmax(torch.sigmoid(linear), dim = 0)\n",
    "        return torch.transpose(softmax, 0, 1)\n",
    "\n",
    "selection_ranker = SelectionRanker()\n",
    "selection_ranker = selection_ranker.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/56355 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ca389cfdbab4751a50cfd466254d316"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in tqdm(data_loader):\n",
    "        req = d[\"request\"]\n",
    "        input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "        attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "        targets = d[\"target\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    "        )\n",
    "        pred_req_id = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets.squeeze(1))\n",
    "        correct_predictions += 1 if pred_req_id == req['sql']['sel'] else 0\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            req = d[\"request\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "            attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "            targets = d[\"target\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids= token_type_ids,\n",
    "            )\n",
    "\n",
    "            pred_req_id = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets.squeeze(1))\n",
    "            correct_predictions += 1 if pred_req_id == req['sql']['sel'] else 0\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    selection_ranker,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train_data_loader)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    selection_ranker,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(val_data_loader)\n",
    "  )\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([1, 6]) torch.Size([1, 6])\n",
      "torch.Size([1, 1]) torch.Size([1])\n",
      "torch.Size([1, 6]) tensor([[0.1645, 0.1733, 0.1844, 0.1563, 0.1611, 0.1605]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([1, 1]) tensor([5])\n",
      "torch.Size([1]) tensor([2])\n"
     ]
    }
   ],
   "source": [
    "d = next(iter(train_data_loader))\n",
    "input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "targets = d[\"target\"].to(device)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = selection_ranker(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    ")\n",
    "\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1).shape)\n",
    "print(targets.shape, targets.squeeze(1).shape)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1))\n",
    "print(targets.shape, targets.squeeze(1))\n",
    "# I dont get why targets is shape [1, 1] and not [1] it is initilized the same way in the data loader\n",
    "\n",
    "target = torch.tensor([2], dtype=torch.long).to(device)\n",
    "print(target.shape, target)\n",
    "\n",
    "\n",
    "loss = loss_fn(outputs, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}