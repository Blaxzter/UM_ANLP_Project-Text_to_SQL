{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ANLP_Project_SeqToSQL.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "fb809b3677754bdcaf6aa222b5e904de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ff93b9f5a7574239a1b225e67802a46a",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_188f24d628464342afb078d56c8a70a3",
       "IPY_MODEL_2a9a368356f44ae9bfa392a3f6b8bc4a"
      ]
     }
    },
    "ff93b9f5a7574239a1b225e67802a46a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "188f24d628464342afb078d56c8a70a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_9d6f0e8ada0e4b539bd87a078f1dbef4",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 213450,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 213450,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_4b942ac9e99f4ff9844a5c5f02f5b520"
     }
    },
    "2a9a368356f44ae9bfa392a3f6b8bc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_cc5fc7560b684aa69ad61777002378e1",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 213k/213k [00:00&lt;00:00, 536kB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_06d9784d6962497580089de9a4af5a67"
     }
    },
    "9d6f0e8ada0e4b539bd87a078f1dbef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "4b942ac9e99f4ff9844a5c5f02f5b520": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "cc5fc7560b684aa69ad61777002378e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "06d9784d6962497580089de9a4af5a67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "07e05e4e2a694c31a2c3bcdb6d5586ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_46acbe4de8594572a276b86454a11602",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_5ac7d0f4bfc64e9e912516cd9894f4ea",
       "IPY_MODEL_ed186036d82743b4bbaeb7b421bb04d8"
      ]
     }
    },
    "46acbe4de8594572a276b86454a11602": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "5ac7d0f4bfc64e9e912516cd9894f4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_13ced24dc1e2472c966d6dbe91be01ab",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 433,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 433,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_86c01d2179d74763b5b957fc942001df"
     }
    },
    "ed186036d82743b4bbaeb7b421bb04d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_fa861b8e35ee440aaef9e38342ab410b",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 433/433 [00:00&lt;00:00, 998B/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_1f43877e3c0d42da8634d6200ab588b1"
     }
    },
    "13ced24dc1e2472c966d6dbe91be01ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "86c01d2179d74763b5b957fc942001df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "fa861b8e35ee440aaef9e38342ab410b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "1f43877e3c0d42da8634d6200ab588b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "246babcbaf074305bd4f888e12fd4b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_7fded0dab11f4fdc985c1c173b7097a3",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_b52e07a6ffd443da868dd2e9225a7eeb",
       "IPY_MODEL_5f4ac871552542e1adc08228c2252256"
      ]
     }
    },
    "7fded0dab11f4fdc985c1c173b7097a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "b52e07a6ffd443da868dd2e9225a7eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_ce72d94b9a7e461fbe45ab6a6ce982da",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 435779157,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 435779157,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2e1647e162c344d5914fd295d06a2914"
     }
    },
    "5f4ac871552542e1adc08228c2252256": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_1531bf8cc3ec4448ab1115b1da49f75f",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 436M/436M [00:08&lt;00:00, 53.0MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6ebf2f22e1f94960bf029a3a25ede4c7"
     }
    },
    "ce72d94b9a7e461fbe45ab6a6ce982da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2e1647e162c344d5914fd295d06a2914": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "1531bf8cc3ec4448ab1115b1da49f75f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "6ebf2f22e1f94960bf029a3a25ede4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73U6yPyZmhGL"
   },
   "source": [
    "# Sequence To SQL\n",
    "\n",
    "Welcome to our project in the Advanced Natural Language Processing course\n",
    "\n",
    "We try to build it with the data provided in https://github.com/salesforce/WikiSQL\n",
    "\n",
    "Remove this: https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?gi=6b6c7e91e298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPUHkkwhVukj",
    "outputId": "d365e360-6c44-4342-f1c7-c32ebe5c8bcc"
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find GPU. Will use CPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection and Review\n",
    "Clone the data from the WikiSQL git repository and install them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMz3n3dbHlU"
   },
   "source": [
    "Take a look inside the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zR6cyK-UbFNV"
   },
   "source": [
    "import json\n",
    "\n",
    "def read_json_data_from_file(file: str):\n",
    "  ret_data = []\n",
    "  with open(file) as json_file:\n",
    "      # Get next line from file\n",
    "      lines = json_file.readlines()\n",
    "      for line in tqdm(lines):\n",
    "          if not line:\n",
    "            break\n",
    "\n",
    "          data = json.loads(line)\n",
    "          ret_data.append(data)\n",
    "  return ret_data\n",
    "\n",
    "def convert_to_id_dict(data, id_key: str):\n",
    "  ret_dict = {}\n",
    "  for element in data:\n",
    "    if id_key in element:\n",
    "      ret_dict[element[id_key]] = element\n",
    "    else:\n",
    "      print(f'Element {element} doenst contain key {id_key}')\n",
    "  return ret_dict"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A80zkDsgqjE"
   },
   "source": [
    "Lets see if we succesfully serialized the data into objects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sihMUsXg-yt",
    "outputId": "8317c04e-70bf-41bf-8f8c-c4ba39c5fd30"
   },
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "print(f'We have {len(dev_req_data)} dev data with {len(dev_table_data)} tables.')\n",
    "print(f'An example Request: ')\n",
    "print(json.dumps(dev_req_data[0], indent=2))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a238e9805cd5459989cdcc7e26bdb6c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2371dc9d96844c37babfd16df75142f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8421 dev data with 2716 tables.\n",
      "An example Request: \n",
      "{\n",
      "  \"phase\": 1,\n",
      "  \"table_id\": \"1-10015132-11\",\n",
      "  \"question\": \"What position does the player who played for butler cc (ks) play?\",\n",
      "  \"sql\": {\n",
      "    \"sel\": 3,\n",
      "    \"conds\": [\n",
      "      [\n",
      "        5,\n",
      "        0,\n",
      "        \"Butler CC (KS)\"\n",
      "      ]\n",
      "    ],\n",
      "    \"agg\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzjw8292zYcE"
   },
   "source": [
    "### The fields represent the following:\n",
    "\n",
    "* `phase`: the phase in which the dataset was collected. We collected WikiSQL in two phases.\n",
    "* `question`: the natural language question written by the worker.\n",
    "* `table_id`: the ID of the table to which this question is addressed.\n",
    "sql: the SQL query corresponding to the question. This has the following *subfields:\n",
    "  * `sel`: the numerical index of the column that is being selected. You can find the actual column from the table.\n",
    "  * `agg`: the numerical index of the aggregation operator that is being used. You can find the actual operator from Query.agg_ops in lib/query.py.\n",
    "  * `conds`: a list of triplets (column_index, operator_index, condition) where:\n",
    "    * `column_index`: the numerical index of the condition column that is being used. You can find the actual column from the table.\n",
    "    * `operator_index`: the numerical index of the condition operator that is being used. You can find the actual operator from Query.cond_ops in lib/query.py.\n",
    "    * `condition`: the comparison value for the condition, in either string or float type.\n",
    "\n",
    "\n",
    "## An example Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f7vfdbCKGG3p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2fb59ef-25a3-4d4f-d652-5f8f0d6b796d"
   },
   "source": [
    "print(json.dumps(dev_table_data[0], indent=2))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"header\": [\n",
      "    \"Player\",\n",
      "    \"No.\",\n",
      "    \"Nationality\",\n",
      "    \"Position\",\n",
      "    \"Years in Toronto\",\n",
      "    \"School/Club Team\"\n",
      "  ],\n",
      "  \"page_title\": \"Toronto Raptors all-time roster\",\n",
      "  \"types\": [\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"id\": \"1-10015132-11\",\n",
      "  \"section_title\": \"L\",\n",
      "  \"caption\": \"L\",\n",
      "  \"rows\": [\n",
      "    [\n",
      "      \"Antonio Lang\",\n",
      "      \"21\",\n",
      "      \"United States\",\n",
      "      \"Guard-Forward\",\n",
      "      \"1999-2000\",\n",
      "      \"Duke\"\n",
      "    ],\n",
      "    [\n",
      "      \"Voshon Lenard\",\n",
      "      \"2\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"2002-03\",\n",
      "      \"Minnesota\"\n",
      "    ],\n",
      "    [\n",
      "      \"Martin Lewis\",\n",
      "      \"32, 44\",\n",
      "      \"United States\",\n",
      "      \"Guard-Forward\",\n",
      "      \"1996-97\",\n",
      "      \"Butler CC (KS)\"\n",
      "    ],\n",
      "    [\n",
      "      \"Brad Lohaus\",\n",
      "      \"33\",\n",
      "      \"United States\",\n",
      "      \"Forward-Center\",\n",
      "      \"1996\",\n",
      "      \"Iowa\"\n",
      "    ],\n",
      "    [\n",
      "      \"Art Long\",\n",
      "      \"42\",\n",
      "      \"United States\",\n",
      "      \"Forward-Center\",\n",
      "      \"2002-03\",\n",
      "      \"Cincinnati\"\n",
      "    ],\n",
      "    [\n",
      "      \"John Long\",\n",
      "      \"25\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"1996-97\",\n",
      "      \"Detroit\"\n",
      "    ],\n",
      "    [\n",
      "      \"Kyle Lowry\",\n",
      "      \"3\",\n",
      "      \"United States\",\n",
      "      \"Guard\",\n",
      "      \"2012-Present\",\n",
      "      \"Villanova\"\n",
      "    ]\n",
      "  ],\n",
      "  \"name\": \"table_10015132_11\"\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of max columns: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 6.79999999999999, 'Token count')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_amount = []\n",
    "for table in dev_table_data:\n",
    "    column_amount.append(len(table['header']))\n",
    "\n",
    "print(f'Amount of max columns: {np.max(column_amount)}')\n",
    "\n",
    "sns.displot(column_amount)\n",
    "plt.xlim([0, 20])\n",
    "plt.xlabel('Token count')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7i0WqeKz2Cy"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "The data is stored with indices but we need the actual column names so saturate the requests with the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gwsK8yDV2_yD"
   },
   "source": [
    "# Transform the data into a dictonary index by the id\n",
    "dev_table_data_dict = convert_to_id_dict(dev_table_data, 'id')"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iIAcsIpd7OpY"
   },
   "source": [
    "# Get the preliminary data\n",
    "# Maybe we want the other idexes also"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rnvuug6C0kaK"
   },
   "source": [
    "def get_table_column(data_list, tables_dict):\n",
    "  ret_list = []\n",
    "  for element in data_list:\n",
    "    current_table = tables_dict[element['table_id']]\n",
    "    columns = current_table['header']\n",
    "    # Replace the index\n",
    "    element['columns'] = columns\n",
    "    element['types'] = current_table['types']\n",
    "    element['sql']['sel_name'] = columns[element['sql']['sel']]\n",
    "\n",
    "    if 'page_title' in current_table:\n",
    "        element['table_name'] = current_table['page_title']\n",
    "    elif 'section_title' in current_table:\n",
    "        element['table_name'] = current_table['section_title']\n",
    "    elif 'caption' in current_table:\n",
    "        element['table_name'] = current_table['caption']\n",
    "    elif 'name' in current_table:\n",
    "        element['table_name'] = current_table['name']\n",
    "\n",
    "    # For the where conditions\n",
    "    for cond in element['sql']['conds']:\n",
    "      cond[0] = columns[cond[0]]\n",
    "    ret_list.append(element)\n",
    "  return ret_list\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cv1VKePBUwf",
    "outputId": "5f52c637-26e3-47b1-c111-cd10b31d1494"
   },
   "source": [
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "dev_prep_req_data = get_table_column(dev_req_data, dev_table_data_dict)\n",
    "\n",
    "\n",
    "print(f'Filed in with the Columns: ')\n",
    "print(json.dumps(dev_prep_req_data[-2], indent=2))\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a4039234c044cefb49ab2b657ce0ddf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e37002c307514c35964efcd50532b764"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filed in with the Columns: \n",
      "{\n",
      "  \"phase\": 2,\n",
      "  \"table_id\": \"2-12601141-1\",\n",
      "  \"question\": \"Which state does Jimmy Quillen represent?\",\n",
      "  \"sql\": {\n",
      "    \"sel\": 2,\n",
      "    \"conds\": [\n",
      "      [\n",
      "        \"Representative\",\n",
      "        0,\n",
      "        \"jimmy quillen\"\n",
      "      ]\n",
      "    ],\n",
      "    \"agg\": 0,\n",
      "    \"sel_name\": \"State\"\n",
      "  },\n",
      "  \"columns\": [\n",
      "    \"Representative\",\n",
      "    \"Years\",\n",
      "    \"State\",\n",
      "    \"Party\",\n",
      "    \"Lifespan\"\n",
      "  ],\n",
      "  \"types\": [\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\",\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"table_name\": \"List of former members of the United States House of Representatives (Q)\"\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# TODO figure out a good padding size or how to do padding correctly\n",
    "\n",
    "def get_question_answers(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    max_len = 0\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        if max_len < len(embedding['input_ids']):\n",
    "            max_len = len(embedding['input_ids'])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_question_answers_def_length(request, tokenizer, pad_max_length):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            truncation = True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list\n",
    "\n",
    "def get_question_answers_for_where_value_def_length(request, tokenizer, pad_max_length):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    cond_dict = {0:\"equal to \", 1:\"less than \", 2:\"more than \", 3:\"OP \"}\n",
    "\n",
    "    #agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "    #cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "    space_token = ' '\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "\n",
    "    conditions = request['sql']['conds']\n",
    "    for i, cond in enumerate(conditions):\n",
    "        column_name = cond[0]\n",
    "        opp_name = cond_dict[cond[1]]\n",
    "        target = cond[2]\n",
    "        value_question = column_name + space_token + opp_name\n",
    "        #print(f\"target:{target}\")\n",
    "        #print(f\"question: {req_question}\")\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            text=value_question,\n",
    "            text_pair=req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            truncation = True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        #print(f\"embedding:{embedding}\")\n",
    "\n",
    "        input_list.append(embedding)\n",
    "        encoded_target = tokenizer.encode(text=str(target),add_special_tokens=True)[1:-1]\n",
    "        startIdx = 0\n",
    "        endIdx = 0\n",
    "        #print(f\"target:{target}\")\n",
    "        #print(f\"target_emb:{encoded_target}\")\n",
    "        sll=len(encoded_target)\n",
    "        for ind in (i for i,e in enumerate(embedding['input_ids']) if e == encoded_target[0]):\n",
    "            if embedding['input_ids'][ind:ind+sll]==encoded_target:\n",
    "                startIdx = ind\n",
    "                endIdx = ind+sll\n",
    "                break\n",
    "        #this is nessecary for some targets are uppercase while in the questions they are lowercase\n",
    "        if startIdx==0 and endIdx==0:\n",
    "            encoded_target = tokenizer.encode(text=str(target).lower(),add_special_tokens=True)[1:-1]\n",
    "            sll=len(encoded_target)\n",
    "            for ind in (i for i,e in enumerate(embedding['input_ids']) if e == encoded_target[0]):\n",
    "                if embedding['input_ids'][ind:ind+sll]==encoded_target:\n",
    "                    startIdx = ind\n",
    "                    endIdx = ind+sll\n",
    "                    break\n",
    "\n",
    "        #print(f\"start:{startIdx}, endidx: {endIdx}\")\n",
    "        target_list.append([startIdx,endIdx])\n",
    "\n",
    "    return input_list, target_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61bcc7dff17f4cd090ea66c025fbd0aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of max tockens: 107\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "token_amount = []\n",
    "for req in tqdm(dev_prep_req_data):\n",
    "    column_question_pairs = get_question_answers(req, tokenizer)\n",
    "    for column_question_pair in column_question_pairs:\n",
    "        token_amount.append(len(column_question_pair['input_ids']))\n",
    "\n",
    "print(f'Amount of max tockens: {np.max(token_amount)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 6.79999999999999, 'Token count')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(token_amount)\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('Token count')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create data loader that transformed the stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Task(Enum):\n",
    "    SELECT = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08c0eda9d9db4c6d9bee04bddc95e8b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4615da00c671482ab724caaabf89adef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8421 dev data with 2716 tables.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WikiSQLDataset(Dataset):\n",
    "\n",
    "    def __init__(self, requests, tokenizer, pad_length):\n",
    "        self.requests = requests\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_length = pad_length\n",
    "\n",
    "        self.req_prepared = []\n",
    "        self.full_requests = []\n",
    "\n",
    "        self.QA_requests = []\n",
    "\n",
    "        for req in requests:\n",
    "            _qa_values, _qa_targets = get_question_answers_for_where_value_def_length(req, self.tokenizer, self.pad_length)\n",
    "            _qa_input_ids = [req_embedding['input_ids'] for req_embedding in _qa_values]\n",
    "            _qa_token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _qa_values]\n",
    "            _qa_attention_mask = [req_embedding['attention_mask'] for req_embedding in _qa_values]\n",
    "\n",
    "            _qa_where_value = torch.tensor(_qa_targets)\n",
    "\n",
    "            _req_embeddings = get_question_answers_def_length(req, self.tokenizer, self.pad_length)\n",
    "            _input_ids = [req_embedding['input_ids'] for req_embedding in _req_embeddings]\n",
    "            _token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _req_embeddings]\n",
    "            _attention_mask = [req_embedding['attention_mask'] for req_embedding in _req_embeddings]\n",
    "\n",
    "            #correct_sel_id = np.zeros((len(req['columns']),1), dtype=int)\n",
    "            #correct_sel_id[req['sql']['sel']][0] = 1\n",
    "\n",
    "            correct_sel_id = np.array([req['sql']['sel']], dtype=int)\n",
    "\n",
    "            #num_agg = 6\n",
    "            #correct_agg_id =  np.zeros((num_agg,1), dtype=int)\n",
    "            #correct_agg_id[req['sql']['agg']][0] = 1\n",
    "\n",
    "            correct_agg_id = np.array([req['sql']['agg']], dtype=int)\n",
    "\n",
    "            #correct_where_conditions = []\n",
    "            # conds = [[colName, OpperationID, value],[colName, OpperationID, value]]\n",
    "\n",
    "\n",
    "            select_target = torch.tensor([correct_sel_id], dtype=torch.long)\n",
    "            select_agg_target = torch.tensor([correct_agg_id], dtype=torch.long)\n",
    "\n",
    "\n",
    "            self.req_prepared.append(dict(\n",
    "                input_ids = torch.tensor(_input_ids),\n",
    "                token_type_ids = torch.tensor(_token_type_ids),\n",
    "                attention_mask = torch.tensor(_attention_mask),\n",
    "\n",
    "                qa_input_ids = torch.tensor(_qa_input_ids),\n",
    "                qa_attention_mask = torch.tensor(_qa_attention_mask),\n",
    "                qa_token_type_ids = torch.tensor(_qa_token_type_ids),\n",
    "\n",
    "                target = dict(\n",
    "                    SELECT = select_target,\n",
    "                    SELECT_AGG = select_agg_target,\n",
    "                    WHERE_VALUE =  torch.tensor(_qa_targets)\n",
    "                )\n",
    "            ))\n",
    "            self.full_requests.append(req)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_full_request_by_id(self, req_id):\n",
    "        return self.full_requests[req_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.req_prepared)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.req_prepared[item]\n",
    "\n",
    "def get_data_loader(data_type, tokenizer, batch_size, filter_data = True, pad_length = 65):\n",
    "    # TODO check if we can use DataLoader with batch size as done in the tutorial\n",
    "    loaded_req = read_json_data_from_file(f'{data_folder}/{data_type}.jsonl')\n",
    "    loaded_tables = read_json_data_from_file(f'{data_folder}/{data_type}.tables.jsonl')\n",
    "    table_data_dict = convert_to_id_dict(loaded_tables, 'id')\n",
    "\n",
    "    prep_req_data = get_table_column(loaded_req, table_data_dict)\n",
    "\n",
    "    if filter_data:\n",
    "        prep_req_data = list(filter(lambda request: len(request['columns']) == 5, prep_req_data))\n",
    "\n",
    "    print(f'We have {len(loaded_req)} {data_type} data with {len(loaded_tables)} tables.')\n",
    "\n",
    "    return DataLoader(\n",
    "        WikiSQLDataset(requests = prep_req_data, tokenizer = tokenizer, pad_length = pad_length),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "data_folder = '../data'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'target'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[[ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0]],\n \n         [[ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0],\n          [ 101, 3087, 5619,  ...,    0,    0,    0]],\n \n         [[ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0]],\n \n         ...,\n \n         [[ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 1842, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0],\n          [ 101, 3087, 1952,  ...,    0,    0,    0]],\n \n         [[ 101, 3087, 6082,  ...,    0,    0,    0],\n          [ 101, 3087, 6082,  ...,    0,    0,    0],\n          [ 101, 3087, 6082,  ...,    0,    0,    0],\n          [ 101, 3087, 6082,  ...,    0,    0,    0],\n          [ 101, 3087, 6082,  ...,    0,    0,    0]],\n \n         [[ 101, 3087,  163,  ...,    0,    0,    0],\n          [ 101, 3087,  163,  ...,    0,    0,    0],\n          [ 101, 1842,  163,  ...,    0,    0,    0],\n          [ 101, 3087,  163,  ...,    0,    0,    0],\n          [ 101, 3087,  163,  ...,    0,    0,    0]]]),\n 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]],\n \n         [[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]],\n \n         [[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]],\n \n         ...,\n \n         [[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]],\n \n         [[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]],\n \n         [[0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0],\n          [0, 0, 0,  ..., 0, 0, 0]]]),\n 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]],\n \n         [[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]],\n \n         [[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]],\n \n         ...,\n \n         [[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]],\n \n         [[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]],\n \n         [[1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0],\n          [1, 1, 1,  ..., 0, 0, 0]]]),\n 'target': {'SELECT': tensor([[[2]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[4]],\n  \n          [[3]],\n  \n          [[3]],\n  \n          [[1]],\n  \n          [[3]],\n  \n          [[4]],\n  \n          [[4]],\n  \n          [[0]],\n  \n          [[4]],\n  \n          [[1]],\n  \n          [[0]],\n  \n          [[2]],\n  \n          [[0]]]),\n  'SELECT_AGG': tensor([[[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[3]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[1]],\n  \n          [[0]],\n  \n          [[0]],\n  \n          [[0]]])}}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in train_data_loader:\n",
    "    try:\n",
    "        #d = next(iterator)\n",
    "        if d[\"input_ids\"].shape[2] != 65 or d[\"input_ids\"].shape[1] != 5:\n",
    "            print(f'input_ids: {d[\"input_ids\"].shape}')\n",
    "        if d[\"attention_mask\"].shape[2] != 65 or d[\"attention_mask\"].shape[1] != 5:\n",
    "            print(f'attention_mask: {d[\"attention_mask\"].shape}')\n",
    "        if d[\"token_type_ids\"].shape[2] != 65 or d[\"token_type_ids\"].shape[1] != 5:\n",
    "            print(f'token_type_ids: {d[\"token_type_ids\"].shape}')\n",
    "        # print(f'target: {d[\"target\"][\"SELECT\"].shape}')\n",
    "    except:\n",
    "        print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test question answer to Build the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnTnkOXH66r"
   },
   "source": [
    "In this section, we will look into **contextual embeddings**.\n",
    "\n",
    "For this we use [**pretrained BERT**](https://www.aclweb.org/anthology/N19-1423.pdf) provided via [HuggingFace](https://huggingface.co/).\n",
    "\n",
    "Let's first install the HuggingFace python package:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ndN0Emuk2K-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# reference is hydranet https://arxiv.org/pdf/2008.04759.pdf\n",
    "# Dp imports here\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Generall model configuration\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "print(dev_prep_req_data[0])\n",
    "for tokens in get_question_answers(dev_prep_req_data[0], tokenizer):\n",
    "    print(tokens)\n",
    "    print(tokenizer.convert_ids_to_tokens(tokens['input_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyLYVYNrC7Pl",
    "outputId": "827654c5-31a4-4e57-c9ef-4760e5cf20eb"
   },
   "source": [
    "req_embeddings = get_question_answers(dev_prep_req_data[-2], tokenizer)\n",
    "print(req_embeddings)\n",
    "input_ids = [req_embedding['input_ids'] for req_embedding in req_embeddings]\n",
    "token_type_ids = [req_embedding['token_type_ids'] for req_embedding in req_embeddings]\n",
    "attention_mask = [req_embedding['attention_mask'] for req_embedding in req_embeddings]"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phase': 1, 'table_id': '1-10015132-11', 'question': 'What position does the player who played for butler cc (ks) play?', 'sql': {'sel': 3, 'conds': [['School/Club Team', 0, 'Butler CC (KS)']], 'agg': 0, 'sel_name': 'Position'}, 'columns': ['Player', 'No.', 'Nationality', 'Position', 'Years in Toronto', 'School/Club Team'], 'types': ['text', 'text', 'text', 'text', 'text', 'text'], 'table_name': 'Toronto Raptors all-time roster'}\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -3, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 5348, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Player', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -2, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1302, 119, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'No', '.', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -2, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1305, 1785, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'National', '##ity', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 18959, 5053, 2116, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Po', '##si', '##tion', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 5848, 1107, 3506, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'Years', 'in', 'Toronto', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]', '[PAD]']\n",
      "{'overflowing_tokens': [], 'num_truncated_tokens': 0, 'input_ids': [101, 3087, 3506, 21196, 5067, 1155, 118, 1159, 9197, 1323, 120, 1998, 2649, 102, 1327, 1700, 1674, 1103, 1591, 1150, 1307, 1111, 23635, 14402, 113, 180, 1116, 114, 1505, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'text', 'Toronto', 'Rap', '##tors', 'all', '-', 'time', 'roster', 'School', '/', 'Club', 'Team', '[SEP]', 'What', 'position', 'does', 'the', 'player', 'who', 'played', 'for', 'butler', 'cc', '(', 'k', '##s', ')', 'play', '?', '[SEP]']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 7725, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 5848, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1426, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': -1, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1786, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}, {'overflowing_tokens': [], 'num_truncated_tokens': 0, 'input_ids': [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 2583, 27894, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 7725, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 5848, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1426, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 1786, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102, 0], [101, 3087, 5619, 1104, 1393, 1484, 1104, 1103, 1244, 1311, 1585, 1104, 5423, 113, 154, 114, 2583, 27894, 102, 5979, 1352, 1674, 4479, 154, 16966, 7836, 4248, 136, 102]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "input_ids_tensor = torch.tensor(input_ids)\n",
    "token_type_ids_tensor = torch.tensor(token_type_ids)\n",
    "attention_mask_tensor = torch.tensor(attention_mask)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids_tensor, # The tokens representing our input text.\n",
    "    attention_mask=token_type_ids_tensor,\n",
    "    token_type_ids=attention_mask_tensor\n",
    ") # The segment IDs to differentiate question from answer_text\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n",
      "torch.Size([5, 29])\n"
     ]
    },
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3507, -0.0581, -0.2104,  ...,  0.1988,  0.0743,  0.1084],\n         [ 0.2660, -0.3266, -0.4412,  ...,  0.4873,  0.4901,  0.0832],\n         [-0.0448, -0.3415, -0.4355,  ...,  0.2672,  0.3468,  0.0902],\n         ...,\n         [ 0.3708, -0.3513, -0.5675,  ...,  0.4988, -0.0255,  0.0643],\n         [ 0.1847, -0.2189, -0.6240,  ...,  0.8887,  0.7120,  0.1169],\n         [ 0.6433, -0.4236,  0.0704,  ..., -0.1497,  0.4813,  0.0973]],\n\n        [[ 0.3460, -0.0871, -0.2241,  ...,  0.1654,  0.0933,  0.1207],\n         [ 0.2659, -0.2906, -0.4186,  ...,  0.4836,  0.4394,  0.0897],\n         [ 0.2941, -0.3711, -0.5376,  ...,  0.2233,  0.6564,  0.2380],\n         ...,\n         [ 0.4445, -0.6749, -0.5239,  ...,  0.1327, -0.2456, -0.4662],\n         [ 0.3296, -0.3824, -0.5856,  ...,  0.4956, -0.0215,  0.0984],\n         [ 0.1314, -0.2563, -0.6348,  ...,  0.8775,  0.6959,  0.1449]]],\n       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5786,  0.5358,  0.9996,  ...,  0.9998, -0.4525,  0.9599],\n        [-0.5832,  0.5481,  0.9997,  ...,  0.9999, -0.4037,  0.9621]],\n       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'last hidden state  : {outputs.last_hidden_state.shape}')\n",
    "print(f'pooled output layer: {outputs.pooler_output.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state  : torch.Size([5, 29, 768])\n",
      "pooled output layer: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# weight = torch.rand(model.config.hidden_size)\n",
    "# F.sigmoid(F.linear(outputs.pooler_output, weight))\n",
    "\n",
    "out = nn.Linear(model.config.hidden_size, 1)\n",
    "out2 = torch.softmax(torch.sigmoid(out(outputs.pooler_output)), dim = 0)\n",
    "\n",
    "torch.argmax(out2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BertBaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertBaseModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.squeeze(0),\n",
    "            attention_mask=attention_mask.squeeze(0),\n",
    "            token_type_ids=token_type_ids.squeeze(0)\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "class SelectionRanker(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SelectionRanker, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output)\n",
    "        softmax = torch.softmax(torch.sigmoid(linear), dim = 0)\n",
    "        return torch.transpose(softmax, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        for d in tepoch:\n",
    "            # req = d[\"request\"]\n",
    "            input_id_batch = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "            attention_mask_batch = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "            token_type_ids_batch = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "            targets_batsch = d[\"target\"]['SELECT'].to(device)\n",
    "\n",
    "            for batchIdx in range(len(input_id_batch)):\n",
    "                input_ids = input_id_batch[batchIdx]# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                attention_mask = attention_mask_batch[batchIdx] # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                token_type_ids = token_type_ids_batch[batchIdx] # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                targets = targets_batsch[batchIdx][0]\n",
    "\n",
    "                #print(f\"input_ids.shape:{input_ids.shape}\")\n",
    "                #print(f\"attention_mask.shape:{attention_mask.shape}\")\n",
    "                #print(f\"token_type_ids .shape:{token_type_ids .shape}\")\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids= token_type_ids,\n",
    "                )\n",
    "                pred_req_id = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                #print(f\"output.shape:{outputs}\")\n",
    "                #print(f\"pred_req_id.shape:{pred_req_id}\")\n",
    "                #print(f\"target.shape:{targets.shape}\")\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                correct_predictions += 1 if pred_req_id == targets else 0\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "            for d in tepoch:\n",
    "                # req = d[\"request\"]\n",
    "                input_id_batch = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                attention_mask_batch = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                token_type_ids_batch = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                targets_batsch = d[\"target\"]['SELECT'].to(device)\n",
    "\n",
    "                for batchIdx in range(len(input_id_batch)):\n",
    "                    input_ids = input_id_batch[batchIdx]# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                    attention_mask = attention_mask_batch[batchIdx] # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                    token_type_ids = token_type_ids_batch[batchIdx] # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                    targets = targets_batsch[batchIdx][0]\n",
    "\n",
    "                    #print(f\"input_ids.shape:{input_ids.shape}\")\n",
    "                    #print(f\"attention_mask.shape:{attention_mask.shape}\")\n",
    "                    #print(f\"token_type_ids .shape:{token_type_ids .shape}\")\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids= token_type_ids,\n",
    "                    )\n",
    "                    pred_req_id = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    #print(f\"output.shape:{outputs}\")\n",
    "                    #print(f\"pred_req_id.shape:{pred_req_id}\")\n",
    "                    #print(f\"target.shape:{targets.shape}\")\n",
    "\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    correct_predictions += 1 if pred_req_id == targets else 0\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "bert_model = BertBaseModel()\n",
    "selection_ranker = SelectionRanker(bert_model)\n",
    "selection_ranker = selection_ranker.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(dev_prep_req_data) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "train_data_loader = get_data_loader(data_type='train', tokenizer = tokenizer, batch_size = 16)\n",
    "val_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)\n",
    "# test_data_loader = get_data_loader('train', tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/56355 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "008909a633004d0495698fb2f5d244a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/18585 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0edb7cf3fdd64c93b52107484a4eaffd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 56355 train data with 18585 tables.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-65045ac00b7d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[0mloss_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNLLLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[0mtrain_data_loader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_data_loader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'train'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[0mval_data_loader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_data_loader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'dev'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;31m# test_data_loader = get_data_loader('train', tokenizer)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-16-a706be97ee52>\u001B[0m in \u001B[0;36mget_data_loader\u001B[1;34m(data_type, tokenizer, batch_size, filter_data, pad_length)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m     return DataLoader(\n\u001B[1;32m---> 74\u001B[1;33m         \u001B[0mWikiSQLDataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrequests\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprep_req_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad_length\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpad_length\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m         \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m     )\n",
      "\u001B[1;32m<ipython-input-16-a706be97ee52>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, requests, tokenizer, pad_length)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m             self.req_prepared.append(dict(\n\u001B[1;32m---> 40\u001B[1;33m                 \u001B[0minput_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_input_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m                 \u001B[0mtoken_type_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_token_type_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m                 \u001B[0mattention_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_attention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(\"started training!\")\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    selection_ranker,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train_data_loader)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    selection_ranker,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(val_data_loader)\n",
    "  )\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8421 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79dd0b4995ea4e918a5fc10981658691"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2716 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3c3cf8b35ed43aeaff80e4d1d6ca00e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8421 dev data with 2716 tables.\n"
     ]
    }
   ],
   "source": [
    "class BertBaseModelBatches(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super(BertBaseModelBatches, self).__init__()\n",
    "        if config:\n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.flatten(end_dim=1),\n",
    "            attention_mask=attention_mask.flatten(end_dim=1),\n",
    "            token_type_ids=token_type_ids.flatten(end_dim=1)\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "class SelectionRankerBatches(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SelectionRankerBatches, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output.view(16, 5, 768))\n",
    "        softmax = torch.softmax(\n",
    "            torch.sigmoid(linear),\n",
    "            dim = 1\n",
    "        )\n",
    "        return softmax\n",
    "\n",
    "bert_model = BertBaseModelBatches()\n",
    "selection_ranker = SelectionRankerBatches(bert_model)\n",
    "\n",
    "d = next(iter(test_data_loader))\n",
    "input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "targets = d[\"target\"][\"SELECT\"].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(token_type_ids.shape)\n",
    "\n",
    "# input_ids.flatten(end_dim=1).view(16,5,65).shape\n",
    "\n",
    "outputs = selection_ranker(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    ")\n",
    "\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1).shape)\n",
    "print(targets.shape, targets.squeeze(1).shape)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1))\n",
    "print(targets.shape, targets.squeeze(1))\n",
    "# I dont get why targets is shape [1, 1] and not [1] it is initilized the same way in the data loader\n",
    "\n",
    "# target = torch.tensor([2], dtype=torch.long).to(device)\n",
    "# print(target.shape, target)\n",
    "\n",
    "\n",
    "loss = loss_fn(outputs, targets.squeeze(1))\n",
    "print(loss)\n",
    "\n",
    "correct_prediction = torch.sum(torch.argmax(outputs, dim=1) == targets.squeeze(1))\n",
    "print(f'Number of correct predictions {correct_prediction}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 65])\n",
      "torch.Size([16, 5, 65])\n",
      "torch.Size([16, 5, 65])\n",
      "torch.Size([16, 5, 1]) torch.Size([16, 5, 1])\n",
      "torch.Size([16, 1, 1]) torch.Size([16, 1])\n",
      "torch.Size([16, 5, 1]) tensor([[[0.2142],\n",
      "         [0.1935],\n",
      "         [0.1999],\n",
      "         [0.1786],\n",
      "         [0.2138]],\n",
      "\n",
      "        [[0.2113],\n",
      "         [0.1953],\n",
      "         [0.1886],\n",
      "         [0.2115],\n",
      "         [0.1933]],\n",
      "\n",
      "        [[0.1864],\n",
      "         [0.1727],\n",
      "         [0.2169],\n",
      "         [0.2152],\n",
      "         [0.2088]],\n",
      "\n",
      "        [[0.1837],\n",
      "         [0.1788],\n",
      "         [0.2003],\n",
      "         [0.1981],\n",
      "         [0.2391]],\n",
      "\n",
      "        [[0.1818],\n",
      "         [0.1963],\n",
      "         [0.2190],\n",
      "         [0.2071],\n",
      "         [0.1957]],\n",
      "\n",
      "        [[0.2019],\n",
      "         [0.2056],\n",
      "         [0.1922],\n",
      "         [0.2183],\n",
      "         [0.1820]],\n",
      "\n",
      "        [[0.2127],\n",
      "         [0.1961],\n",
      "         [0.2118],\n",
      "         [0.1888],\n",
      "         [0.1906]],\n",
      "\n",
      "        [[0.2118],\n",
      "         [0.2067],\n",
      "         [0.1867],\n",
      "         [0.2151],\n",
      "         [0.1796]],\n",
      "\n",
      "        [[0.1722],\n",
      "         [0.1910],\n",
      "         [0.2251],\n",
      "         [0.2124],\n",
      "         [0.1993]],\n",
      "\n",
      "        [[0.1853],\n",
      "         [0.2175],\n",
      "         [0.1778],\n",
      "         [0.1822],\n",
      "         [0.2372]],\n",
      "\n",
      "        [[0.1892],\n",
      "         [0.2141],\n",
      "         [0.1935],\n",
      "         [0.2043],\n",
      "         [0.1988]],\n",
      "\n",
      "        [[0.1842],\n",
      "         [0.1845],\n",
      "         [0.1942],\n",
      "         [0.2373],\n",
      "         [0.1999]],\n",
      "\n",
      "        [[0.1905],\n",
      "         [0.2133],\n",
      "         [0.1783],\n",
      "         [0.2110],\n",
      "         [0.2070]],\n",
      "\n",
      "        [[0.1953],\n",
      "         [0.2143],\n",
      "         [0.1987],\n",
      "         [0.2046],\n",
      "         [0.1871]],\n",
      "\n",
      "        [[0.2025],\n",
      "         [0.1763],\n",
      "         [0.2174],\n",
      "         [0.1979],\n",
      "         [0.2059]],\n",
      "\n",
      "        [[0.1966],\n",
      "         [0.2254],\n",
      "         [0.1780],\n",
      "         [0.2174],\n",
      "         [0.1826]]], grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([16, 1, 1]) tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [4],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]])\n",
      "tensor(-0.2076, grad_fn=<NllLoss2DBackward>)\n",
      "Number of correct predictions 6\n"
     ]
    }
   ],
   "source": [
    "class QABert(nn.Module):\n",
    "    def __init__(self, bert_model, output_num_words):\n",
    "        super(QABert, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linearStart = nn.Linear(self.bert.bert.config.hidden_size, output_num_words)\n",
    "        self.linearEnd= nn.Linear(self.bert.bert.config.hidden_size, output_num_words)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        startValues = self.linearStart(outputs.last_hidden_state)\n",
    "        startSoftmax = torch.softmax(startValues, dim=1)\n",
    "\n",
    "        endValues = self.linearEnd(outputs.last_hidden_state)\n",
    "        endSoftmax = torch.softmax(endValues, dim=1)\n",
    "\n",
    "        return startSoftmax, endSoftmax\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'\n",
    "\n",
    "d = next(iter(train_data_loader))\n",
    "input_ids = d[\"input_ids\"][0].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"][0].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"][0].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "\n",
    "\n",
    "\n",
    "#Tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#question = '''wage less than '''\n",
    "#paragraph = '''show please me all employees with a salary smaller than 400 or where the last name is \"yannick\"'''\n",
    "\n",
    "#encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True)\n",
    "\n",
    "#inputs = torch.tensor(encoding['input_ids'])  #Token embeddings\n",
    "#sentence_embedding = torch.tensor(encoding['token_type_ids'])  #Segment embeddings\n",
    "#attention_mask = torch.tensor(encoding['attention_mask'])\n",
    "\n",
    "\n",
    "modelWV = QABert(bert_model,1)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(token_type_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "\n",
    "output = modelWV(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "start_scores = output[0]\n",
    "print(f\"start_scores: {start_scores.shape}\")\n",
    "end_scores = output[1]\n",
    "\n",
    "start_index = torch.argmax(start_scores, dim=1)\n",
    "\n",
    "print(f\"start_index{start_index}\")\n",
    "\n",
    "end_index = torch.argmax(end_scores, dim=1)\n",
    "\n",
    "answer = ' '.join(tokens[start_index:end_index+1])\n",
    "print(answer)\n",
    "corrected_answer = ''\n",
    "\n",
    "for word in answer.split():\n",
    "    #If it's a subword token\n",
    "    if word[0:2] == '##':\n",
    "        corrected_answer += word[2:]\n",
    "    else:\n",
    "        corrected_answer += ' ' + word\n",
    "print(corrected_answer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}