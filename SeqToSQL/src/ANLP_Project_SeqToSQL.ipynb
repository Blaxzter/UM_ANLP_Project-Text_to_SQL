{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73U6yPyZmhGL"
   },
   "source": [
    "# Sequence To SQL\n",
    "\n",
    "Welcome to our project in the Advanced Natural Language Processing course\n",
    "\n",
    "We try to build it with the data provided in https://github.com/salesforce/WikiSQL\n",
    "\n",
    "Remove this: https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?gi=6b6c7e91e298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPUHkkwhVukj",
    "outputId": "d365e360-6c44-4342-f1c7-c32ebe5c8bcc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and Review\n",
    "Clone the data from the WikiSQL git repository and install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure you are in the /src folder for the next cell to work properly\n",
    "!git clone https://github.com/salesforce/WikiSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsyB_6W1ZY_Z",
    "outputId": "fcb1ae53-feb4-4c10-82ce-77d9d591807b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_folder = '../data'\n",
    "data_sql_dir = os.path.isdir(data_folder)\n",
    "\n",
    "if not data_sql_dir:\n",
    "  %cd ..\n",
    "  !git clone https://github.com/salesforce/WikiSQL\n",
    "  %cd WikiSQL\n",
    "  !tar xvjf data.tar.bz2\n",
    "  !move data ../\n",
    "  %cd ..\n",
    "\n",
    "print(\"Data available\")\n",
    "print(os.listdir(data_folder))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMz3n3dbHlU"
   },
   "source": [
    "Take a look inside the data\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtK-MsXReB8N",
    "outputId": "bcaf2bf6-26df-4841-f009-db1bd56e233a"
   },
   "outputs": [],
   "source": [
    "!head -5 data/dev.jsonl\r\n",
    "!head -5 data/dev.tables.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR6cyK-UbFNV"
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "\r\n",
    "def read_json_data_from_file(file: str):\r\n",
    "  ret_data = []\r\n",
    "  with open(file) as json_file:\r\n",
    "    while True:\r\n",
    "      # Get next line from file\r\n",
    "      line = json_file.readline()\r\n",
    "      if not line:\r\n",
    "        break\r\n",
    "\r\n",
    "      data = json.loads(line)\r\n",
    "      ret_data.append(data)\r\n",
    "  return ret_data\r\n",
    "\r\n",
    "def convert_to_id_dict(data, id_key: str):\r\n",
    "  ret_dict = {}\r\n",
    "  for element in data:\r\n",
    "    if id_key in element:\r\n",
    "      ret_dict[element[id_key]] = element\r\n",
    "    else:\r\n",
    "      print(f'Element {element} doenst contain key {id_key}')\r\n",
    "  return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A80zkDsgqjE"
   },
   "source": [
    "Lets see if we succesfully serialized the data into objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sihMUsXg-yt",
    "outputId": "8317c04e-70bf-41bf-8f8c-c4ba39c5fd30"
   },
   "outputs": [],
   "source": [
    "dev_req_data = read_json_data_from_file('data/dev.jsonl')\r\n",
    "dev_table_data = read_json_data_from_file('data/dev.tables.jsonl')\r\n",
    " \r\n",
    "print(f'We have {len(dev_req_data)} dev data with {len(dev_table_data)} tables.')\r\n",
    "print(f'An example Request: ')\r\n",
    "print(json.dumps(dev_req_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzjw8292zYcE"
   },
   "source": [
    "### The fields represent the following:\r\n",
    "\r\n",
    "* `phase`: the phase in which the dataset was collected. We collected WikiSQL in two phases.\r\n",
    "* `question`: the natural language question written by the worker.\r\n",
    "* `table_id`: the ID of the table to which this question is addressed.\r\n",
    "sql: the SQL query corresponding to the question. This has the following *subfields:\r\n",
    "  * `sel`: the numerical index of the column that is being selected. You can find the actual column from the table.\r\n",
    "  * `agg`: the numerical index of the aggregation operator that is being used. You can find the actual operator from Query.agg_ops in lib/query.py.\r\n",
    "  * `conds`: a list of triplets (column_index, operator_index, condition) where:\r\n",
    "    * `column_index`: the numerical index of the condition column that is being used. You can find the actual column from the table.\r\n",
    "    * `operator_index`: the numerical index of the condition operator that is being used. You can find the actual operator from Query.cond_ops in lib/query.py.\r\n",
    "    * `condition`: the comparison value for the condition, in either string or float type.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7vfdbCKGG3p",
    "outputId": "a2fb59ef-25a3-4d4f-d652-5f8f0d6b796d"
   },
   "outputs": [],
   "source": [
    "print(f'An example Table: ')\r\n",
    "print(json.dumps(dev_table_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7i0WqeKz2Cy"
   },
   "source": [
    "## Preprocess\r\n",
    "\r\n",
    "The data is stored with indices but we need the actual column names so saturate the requests with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwsK8yDV2_yD"
   },
   "outputs": [],
   "source": [
    "# Transform the data into a dictonary index by the id\r\n",
    "dev_table_data_dict = convert_to_id_dict(dev_table_data, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIAcsIpd7OpY"
   },
   "outputs": [],
   "source": [
    "# Get the preliminary data\r\n",
    "# Maybe we want the other idexes also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rnvuug6C0kaK"
   },
   "outputs": [],
   "source": [
    "def get_table_column(data_list, tables_dict):\r\n",
    "  ret_list = []\r\n",
    "  for element in data_list:\r\n",
    "    current_table = tables_dict[element['table_id']]\r\n",
    "    columns = current_table['header']\r\n",
    "    # Replace the index\r\n",
    "    element['columns'] = columns\r\n",
    "    element['sql']['sel'] = columns[element['sql']['sel']]\r\n",
    "\r\n",
    "    # For the where conditions\r\n",
    "    for cond in element['sql']['conds']:\r\n",
    "      cond[0] = columns[cond[0]]\r\n",
    "    ret_list.append(element)\r\n",
    "  return ret_list\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cv1VKePBUwf",
    "outputId": "5f52c637-26e3-47b1-c111-cd10b31d1494"
   },
   "outputs": [],
   "source": [
    "dev_req_data = read_json_data_from_file('data/dev.jsonl')\r\n",
    "dev_table_data = read_json_data_from_file('data/dev.tables.jsonl')\r\n",
    "dev_prep_req_data = get_table_column(dev_req_data, dev_table_data_dict)\r\n",
    "print(f'Filed in with the Columns: ')\r\n",
    "print(json.dumps(dev_prep_req_data[-2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnTnkOXH66r"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYam0Qa_R13y",
    "outputId": "50c1ffa1-2113-4b08-8d32-b4dadc80aa6a"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ndN0Emuk2K-"
   },
   "source": [
    "In this section, we will look into **contextual embeddings**. \n",
    "\n",
    "For this we use [**pretrained BERT**](https://www.aclweb.org/anthology/N19-1423.pdf) provided via [HuggingFace](https://huggingface.co/).\n",
    "\n",
    "Let's first install the HuggingFace python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180,
     "referenced_widgets": [
      "fb809b3677754bdcaf6aa222b5e904de",
      "ff93b9f5a7574239a1b225e67802a46a",
      "188f24d628464342afb078d56c8a70a3",
      "2a9a368356f44ae9bfa392a3f6b8bc4a",
      "9d6f0e8ada0e4b539bd87a078f1dbef4",
      "4b942ac9e99f4ff9844a5c5f02f5b520",
      "cc5fc7560b684aa69ad61777002378e1",
      "06d9784d6962497580089de9a4af5a67",
      "07e05e4e2a694c31a2c3bcdb6d5586ab",
      "46acbe4de8594572a276b86454a11602",
      "5ac7d0f4bfc64e9e912516cd9894f4ea",
      "ed186036d82743b4bbaeb7b421bb04d8",
      "13ced24dc1e2472c966d6dbe91be01ab",
      "86c01d2179d74763b5b957fc942001df",
      "fa861b8e35ee440aaef9e38342ab410b",
      "1f43877e3c0d42da8634d6200ab588b1",
      "246babcbaf074305bd4f888e12fd4b78",
      "7fded0dab11f4fdc985c1c173b7097a3",
      "b52e07a6ffd443da868dd2e9225a7eeb",
      "5f4ac871552542e1adc08228c2252256",
      "ce72d94b9a7e461fbe45ab6a6ce982da",
      "2e1647e162c344d5914fd295d06a2914",
      "1531bf8cc3ec4448ab1115b1da49f75f",
      "6ebf2f22e1f94960bf029a3a25ede4c7"
     ]
    },
    "id": "67QerZklRzEP",
    "outputId": "907ac132-f65a-403c-f54a-436261fa545d"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\r\n",
    "\r\n",
    "sent = 'I left my phone in my left pocket'\r\n",
    "\r\n",
    "encoded_input = tokenizer(sent, add_special_tokens=True, return_tensors='pt')\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    output = model(**encoded_input)\r\n",
    "\r\n",
    "print(output.last_hidden_state.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyLYVYNrC7Pl",
    "outputId": "827654c5-31a4-4e57-c9ef-4760e5cf20eb"
   },
   "outputs": [],
   "source": [
    "dev_table_data_dict['2-1123478-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeujv-VJIHGy"
   },
   "outputs": [],
   "source": [
    "# reference is hydranet https://arxiv.org/pdf/2008.04759.pdf\r\n",
    "\r\n",
    "def get_input_vetor(request):\r\n",
    "\r\n",
    "  input_list = []\r\n",
    "  \r\n",
    "  c_table = dev_table_data_dict[request['table_id']]\r\n",
    "\r\n",
    "  table_name = c_table['caption'] #should be name not id\r\n",
    "  sep_token = '[SEP]'\r\n",
    "  space_token = ' '\r\n",
    "  cls_token = '[CLS]'\r\n",
    "  columns = request['columns']\r\n",
    "  question = request['question'] # might need to be tokenized\r\n",
    "  for i, col in enumerate(columns):\r\n",
    "    col_type = c_table['types'][i] # infere type somehow\r\n",
    "    token_seq = cls_token + col_type + space_token + table_name + space_token + col + sep_token + question + sep_token\r\n",
    "    embedded_token_seq = token_seq # embedding here ?! \r\n",
    "    input_list.append(embedded_token_seq)\r\n",
    "\r\n",
    "  return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IlOcU6bCder",
    "outputId": "007813ed-52de-43b1-c00e-d09cbe9aec45"
   },
   "outputs": [],
   "source": [
    "get_input_vetor(dev_prep_req_data[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_qge_QPAGDN",
    "outputId": "e00d40a4-b2c2-4e25-817a-720fb3e5f179"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice\r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
    "[tokenizer.tokenize(input_text) for input_text in get_input_vetor(dev_prep_req_data[-2])]\r\n",
    "\r\n",
    "model = BertForMultipleChoice.from_pretrained(\"bert-base-cased\")\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeSXQLygIe3c",
    "outputId": "8449458d-921f-45a5-f8dc-455c593be516"
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import MobileBertTokenizer, MobileBertForMultipleChoice\r\n",
    "\r\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\r\n",
    "model = MobileBertForMultipleChoice.from_pretrained('google/mobilebert-uncased')\r\n",
    "\r\n",
    "fact_sent = \"Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries.\"\r\n",
    "answer_1 = \"It has become one of the most popular foods in the world\"\r\n",
    "answer_2 = \"It has gone nowhere from there\"\r\n",
    "\r\n",
    "labels = torch.tensor(0).unsqueeze(0)\r\n",
    "encoding = tokenizer([[fact_sent, fact_sent], [answer_1, answer_2]], return_tensors='pt', padding=True)\r\n",
    "\r\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)\r\n",
    "\r\n",
    "loss, logits = outputs[:2]\r\n",
    "\r\n",
    "softmax = nn.Softmax(dim=1)\r\n",
    "probabilities = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhRZoXbZI1GO",
    "outputId": "9caf5424-b706-45b6-82c2-6b1872182c18"
   },
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiVXECwsLhu0"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\r\n",
    "from transformers import BertForTokenClassification\r\n",
    "from transformers import BertConfig\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
    "[tokenizer.tokenize(input_text) for input_text in get_input_vetor(dev_prep_req_data[-2])]\r\n",
    "\r\n",
    "model = BertForMultipleChoice.from_pretrained(\"bert-base-cased\")\r\n",
    "\r\n",
    "# Show the model configuration\r\n",
    "print(model.config)\r\n",
    "\r\n",
    "# Optimer has relatively low learning rate as we are finetuning a trained model.\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\r\n",
    "\r\n",
    "training_data = dataset_subword.data['train_subword']\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "validation_batch_size = 16\r\n",
    "epochs=1    # Total number of epochs\r\n",
    "report_every=5000   # Report validation performance after every X sentences\r\n",
    "\r\n",
    "for epoch in range(1, epochs+1):\r\n",
    "    print('Epoch', epoch)\r\n",
    "    \r\n",
    "    sent_cnt = 0\r\n",
    "    text_batch = []\r\n",
    "    label_batch = []\r\n",
    "    max_len = 0\r\n",
    "\r\n",
    "    for _, targets, full_sent in training_data:\r\n",
    "        sent_cnt += 1\r\n",
    "        # Accumulate sentences and label sequences for the current batch\r\n",
    "        text_batch.append(full_sent)\r\n",
    "        label_batch.append(targets)\r\n",
    "        max_len = max(max_len, len(targets))\r\n",
    "\r\n",
    "        if len(text_batch) == batch_size or sent_cnt == len(training_data)-1:\r\n",
    "            # Tokenize the batch of input sentences\r\n",
    "            encoding = tokenizer(text_batch, return_tensors='pt', add_special_tokens=False, padding=True, truncation=True).to(device)\r\n",
    "            # Pad the the labels to the maximum length in the current batch.\r\n",
    "            # (No need to pad input sentences as that's handled in the tokenzier by `padding=True`)\r\n",
    "            for i, label_list in enumerate(label_batch):\r\n",
    "                for _ in range(max_len - len(label_list)):\r\n",
    "                    label_batch[i].append(0)\r\n",
    "            label_batch=torch.tensor(label_batch).to(device)\r\n",
    "\r\n",
    "            # Get model output\r\n",
    "            outputs = model(encoding.input_ids, encoding.attention_mask, labels=label_batch)\r\n",
    "            # Calculate loss and gradients\r\n",
    "            loss = outputs.loss\r\n",
    "            loss.backward()\r\n",
    "            # Update model parameter\r\n",
    "            optimizer.step()\r\n",
    "            # Clear gradients (important!)\r\n",
    "            model.zero_grad()\r\n",
    "            # Reset batch-related vairiables\r\n",
    "            text_batch = []\r\n",
    "            label_batch = []\r\n",
    "            max_len = 0\r\n",
    "        \r\n",
    "        # Run on validation set and report performance\r\n",
    "        if (sent_cnt % report_every) == 0 or sent_cnt == len(training_data)-1:\r\n",
    "            model.eval()\r\n",
    "            correct_token, pred_token, true_token = 0.0, 0.0, 0.0\r\n",
    "            print('Epoch {0}: trained {1} sentences'.format(epoch, sent_cnt))\r\n",
    "            \r\n",
    "            val_sent_cnt = 0\r\n",
    "            val_text_batch = []\r\n",
    "            val_label_batch = []\r\n",
    "            val_max_len = 0\r\n",
    "\r\n",
    "            for _, targets, full_sent in dataset_subword.data['valid_subword']:\r\n",
    "                val_sent_cnt += 1\r\n",
    "                # Accumulate sentences and label sequences for the current batch\r\n",
    "                val_text_batch.append(full_sent)\r\n",
    "                val_label_batch.append(targets)\r\n",
    "                val_max_len = max(val_max_len, len(targets))\r\n",
    "\r\n",
    "                if len(val_text_batch) == validation_batch_size or val_sent_cnt == len(dataset_subword.data['valid_subword'])-1:\r\n",
    "                    # Tokenize the batch of input sentences\r\n",
    "                    encoding = tokenizer(val_text_batch, return_tensors='pt', add_special_tokens=False, padding=True, truncation=True).to(device)\r\n",
    "                    for i, label_list in enumerate(val_label_batch):\r\n",
    "                        for _ in range(val_max_len - len(label_list)):\r\n",
    "                            val_label_batch[i].append(0)\r\n",
    "                    torch.tensor(val_label_batch)\r\n",
    "                    val_label_batch=torch.tensor(val_label_batch).to(device)\r\n",
    "                    outputs = model(**encoding)\r\n",
    "\r\n",
    "                    # Go from logits to predicted labels by taking argmax\r\n",
    "                    predictions = torch.argmax(outputs.logits, dim=2)\r\n",
    "                    correct_token += torch.sum((val_label_batch != 0) & (predictions == val_label_batch))\r\n",
    "                    pred_token += torch.sum((val_label_batch != 0))\r\n",
    "                    true_token += torch.sum(val_label_batch != 0)\r\n",
    "                    # Reset batch running variables\r\n",
    "                    val_text_batch = []\r\n",
    "                    val_label_batch = []\r\n",
    "                    val_max_len = 0\r\n",
    "\r\n",
    "            # After seeing all sentences in validation set, calculate precision, recall, F1\r\n",
    "            precision = correct_token / pred_token if pred_token else 0.0\r\n",
    "            recall = correct_token / true_token if true_token else 0.0\r\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\r\n",
    "            print('Epoch {0}:\\t validation precision\\t {1:.1%}; recall\\t {2:.1%}; F1\\t {3:.1%}.'.format(epoch, precision, recall, f1))\r\n",
    "            model.train()\r\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WN8C9DJbLoaO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ANLP_Project_SeqToSQL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
