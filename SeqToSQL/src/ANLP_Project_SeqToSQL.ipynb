{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73U6yPyZmhGL"
   },
   "source": [
    "# Sequence To SQL\n",
    "\n",
    "Welcome to our project in the Advanced Natural Language Processing course\n",
    "\n",
    "We try to build it with the data provided in https://github.com/salesforce/WikiSQL\n",
    "\n",
    "Remove this: https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?gi=6b6c7e91e298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPUHkkwhVukj",
    "outputId": "d365e360-6c44-4342-f1c7-c32ebe5c8bcc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and Review\n",
    "Clone the data from the WikiSQL git repository and install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMz3n3dbHlU"
   },
   "source": [
    "Take a look inside the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR6cyK-UbFNV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_data_from_file(file: str):\n",
    "  ret_data = []\n",
    "  with open(file) as json_file:\n",
    "      # Get next line from file\n",
    "      lines = json_file.readlines()\n",
    "      for line in tqdm(lines):\n",
    "          if not line:\n",
    "            break\n",
    "\n",
    "          data = json.loads(line)\n",
    "          ret_data.append(data)\n",
    "  return ret_data\n",
    "\n",
    "def convert_to_id_dict(data, id_key: str):\n",
    "  ret_dict = {}\n",
    "  for element in data:\n",
    "    if id_key in element:\n",
    "      ret_dict[element[id_key]] = element\n",
    "    else:\n",
    "      print(f'Element {element} doenst contain key {id_key}')\n",
    "  return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A80zkDsgqjE"
   },
   "source": [
    "Lets see if we succesfully serialized the data into objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sihMUsXg-yt",
    "outputId": "8317c04e-70bf-41bf-8f8c-c4ba39c5fd30"
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    " \n",
    "print(f'We have {len(dev_req_data)} dev data with {len(dev_table_data)} tables.')\n",
    "print(f'An example Request: ')\n",
    "print(json.dumps(dev_req_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzjw8292zYcE"
   },
   "source": [
    "### The fields represent the following:\n",
    "\n",
    "* `phase`: the phase in which the dataset was collected. We collected WikiSQL in two phases.\n",
    "* `question`: the natural language question written by the worker.\n",
    "* `table_id`: the ID of the table to which this question is addressed.\n",
    "sql: the SQL query corresponding to the question. This has the following *subfields:\n",
    "  * `sel`: the numerical index of the column that is being selected. You can find the actual column from the table.\n",
    "  * `agg`: the numerical index of the aggregation operator that is being used. You can find the actual operator from Query.agg_ops in lib/query.py.\n",
    "  * `conds`: a list of triplets (column_index, operator_index, condition) where:\n",
    "    * `column_index`: the numerical index of the condition column that is being used. You can find the actual column from the table.\n",
    "    * `operator_index`: the numerical index of the condition operator that is being used. You can find the actual operator from Query.cond_ops in lib/query.py.\n",
    "    * `condition`: the comparison value for the condition, in either string or float type.\n",
    "\n",
    "\n",
    "## An example Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7vfdbCKGG3p",
    "outputId": "a2fb59ef-25a3-4d4f-d652-5f8f0d6b796d"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(dev_table_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7i0WqeKz2Cy"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "The data is stored with indices but we need the actual column names so saturate the requests with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwsK8yDV2_yD"
   },
   "outputs": [],
   "source": [
    "# Transform the data into a dictonary index by the id\n",
    "dev_table_data_dict = convert_to_id_dict(dev_table_data, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIAcsIpd7OpY"
   },
   "outputs": [],
   "source": [
    "# Get the preliminary data\n",
    "# Maybe we want the other idexes also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rnvuug6C0kaK"
   },
   "outputs": [],
   "source": [
    "def get_table_column(data_list, tables_dict):\n",
    "  ret_list = []\n",
    "  for element in data_list:\n",
    "    current_table = tables_dict[element['table_id']]\n",
    "    columns = current_table['header']\n",
    "    # Replace the index\n",
    "    element['columns'] = columns\n",
    "    element['types'] = current_table['types']\n",
    "    element['sql']['sel_name'] = columns[element['sql']['sel']]\n",
    "\n",
    "    if 'page_title' in current_table:\n",
    "        element['table_name'] = current_table['page_title']\n",
    "    elif 'section_title' in current_table:\n",
    "        element['table_name'] = current_table['section_title']\n",
    "    elif 'caption' in current_table:\n",
    "        element['table_name'] = current_table['caption']\n",
    "    elif 'name' in current_table:\n",
    "        element['table_name'] = current_table['name']\n",
    "\n",
    "    # For the where conditions\n",
    "    for cond in element['sql']['conds']:\n",
    "      cond[0] = columns[cond[0]]\n",
    "    ret_list.append(element)\n",
    "  return ret_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cv1VKePBUwf",
    "outputId": "5f52c637-26e3-47b1-c111-cd10b31d1494"
   },
   "outputs": [],
   "source": [
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "dev_prep_req_data = get_table_column(dev_req_data, dev_table_data_dict)\n",
    "\n",
    "\n",
    "print(f'Filed in with the Columns: ')\n",
    "print(json.dumps(dev_prep_req_data[-2], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO figure out a good padding size or how to do padding correctly\n",
    "\n",
    "def get_question_answers(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    max_len = 0\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        if max_len < len(embedding['input_ids']):\n",
    "            max_len = len(embedding['input_ids'])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pad_max_length = 50\n",
    "def get_question_answers_def_length(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create data loader that transformed the stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Task(Enum):\n",
    "    SELECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WikiSQLDataset(Dataset):\n",
    "\n",
    "    def __init__(self, requests, tokenizer, task: Task):\n",
    "        self.requests = requests\n",
    "        self.tokenizer = tokenizer\n",
    "        self.task = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.requests)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        req = self.requests[item]\n",
    "        _req_embeddings = get_question_answers(req, self.tokenizer)\n",
    "        _input_ids = [req_embedding['input_ids'] for req_embedding in _req_embeddings]\n",
    "        _token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _req_embeddings]\n",
    "        _attention_mask = [req_embedding['attention_mask'] for req_embedding in _req_embeddings]\n",
    "\n",
    "        target = None\n",
    "        if self.task == Task.SELECT:\n",
    "            correct_sel_id = req['sql']['sel']\n",
    "            target = torch.tensor([correct_sel_id], dtype=torch.long)\n",
    "\n",
    "        return dict(\n",
    "            request = self.requests[item],\n",
    "            input_ids = torch.tensor(_input_ids),\n",
    "            token_type_ids = torch.tensor(_token_type_ids),\n",
    "            attention_mask = torch.tensor(_attention_mask),\n",
    "            target = target\n",
    "        )\n",
    "\n",
    "def get_data_loader(data_type, tokenizer, task, batch_size):\n",
    "    # TODO check if we can use DataLoader with batch size as done in the tutorial\n",
    "    loaded_req = read_json_data_from_file(f'{data_folder}/{data_type}.jsonl')\n",
    "    loaded_tables = read_json_data_from_file(f'{data_folder}/{data_type}.tables.jsonl')\n",
    "    table_data_dict = convert_to_id_dict(loaded_tables, 'id')\n",
    "\n",
    "    prep_req_data = get_table_column(loaded_req, table_data_dict)\n",
    "\n",
    "    print(f'We have {len(loaded_req)} {data_type} data with {len(loaded_tables)} tables.')\n",
    "\n",
    "    return DataLoader(\n",
    "        WikiSQLDataset(requests = prep_req_data, tokenizer = tokenizer, task=task),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "data_folder = '../data'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "\n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnTnkOXH66r"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ndN0Emuk2K-"
   },
   "source": [
    "In this section, we will look into **contextual embeddings**. \n",
    "\n",
    "For this we use [**pretrained BERT**](https://www.aclweb.org/anthology/N19-1423.pdf) provided via [HuggingFace](https://huggingface.co/).\n",
    "\n",
    "Let's first install the HuggingFace python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dp imports here\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyLYVYNrC7Pl",
    "outputId": "827654c5-31a4-4e57-c9ef-4760e5cf20eb"
   },
   "outputs": [],
   "source": [
    "print(dev_prep_req_data[0])\n",
    "for tokens in get_question_answers(dev_prep_req_data[0], tokenizer):\n",
    "    print(tokens)\n",
    "    print(tokenizer.convert_ids_to_tokens(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generall model configuration\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeujv-VJIHGy"
   },
   "outputs": [],
   "source": [
    "# reference is hydranet https://arxiv.org/pdf/2008.04759.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "req_embeddings = get_question_answers(dev_prep_req_data[-2], tokenizer)\n",
    "print(req_embeddings)\n",
    "input_ids = [req_embedding['input_ids'] for req_embedding in req_embeddings]\n",
    "token_type_ids = [req_embedding['token_type_ids'] for req_embedding in req_embeddings]\n",
    "attention_mask = [req_embedding['attention_mask'] for req_embedding in req_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_ids_tensor = torch.tensor(input_ids)\n",
    "token_type_ids_tensor = torch.tensor(token_type_ids)\n",
    "attention_mask_tensor = torch.tensor(attention_mask)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids_tensor, # The tokens representing our input text.\n",
    "    attention_mask=token_type_ids_tensor,\n",
    "    token_type_ids=attention_mask_tensor\n",
    ") # The segment IDs to differentiate question from answer_text\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'last hidden state  : {outputs.last_hidden_state.shape}')\n",
    "print(f'pooled output layer: {outputs.pooler_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# weight = torch.rand(model.config.hidden_size)\n",
    "# F.sigmoid(F.linear(outputs.pooler_output, weight))\n",
    "\n",
    "out = nn.Linear(model.config.hidden_size, 1)\n",
    "out2 = torch.softmax(torch.sigmoid(out(outputs.pooler_output)), dim = 0)\n",
    "\n",
    "torch.argmax(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(dev_prep_req_data) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data_loader = get_data_loader(data_type='train', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "val_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, task = Task.SELECT, batch_size = 1)\n",
    "# test_data_loader = get_data_loader('train', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SelectionRanker(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SelectionRanker, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.squeeze(0),\n",
    "            attention_mask=attention_mask.squeeze(0),\n",
    "            token_type_ids=token_type_ids.squeeze(0)\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output)\n",
    "        softmax = torch.softmax(torch.sigmoid(linear), dim = 0)\n",
    "        return torch.transpose(softmax, 0, 1)\n",
    "\n",
    "selection_ranker = SelectionRanker()\n",
    "selection_ranker = selection_ranker.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in tqdm(data_loader):\n",
    "        req = d[\"request\"]\n",
    "        input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "        attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "        targets = d[\"target\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    "        )\n",
    "        pred_req_id = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets.squeeze(1))\n",
    "        correct_predictions += 1 if pred_req_id == req['sql']['sel'] else 0\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            req = d[\"request\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "            attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "            targets = d[\"target\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids= token_type_ids,\n",
    "            )\n",
    "\n",
    "            pred_req_id = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets.squeeze(1))\n",
    "            correct_predictions += 1 if pred_req_id == req['sql']['sel'] else 0\n",
    "            losses.append(loss.item())\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    selection_ranker,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train_data_loader)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    selection_ranker,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(val_data_loader)\n",
    "  )\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = next(iter(train_data_loader))\n",
    "input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "targets = d[\"target\"].to(device)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = selection_ranker(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    ")\n",
    "\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1).shape)\n",
    "print(targets.shape, targets.squeeze(1).shape)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1))\n",
    "print(targets.shape, targets.squeeze(1))\n",
    "# I dont get why targets is shape [1, 1] and not [1] it is initilized the same way in the data loader\n",
    "\n",
    "target = torch.tensor([2], dtype=torch.long).to(device)\n",
    "print(target.shape, target)\n",
    "\n",
    "\n",
    "loss = loss_fn(outputs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ANLP_Project_SeqToSQL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
