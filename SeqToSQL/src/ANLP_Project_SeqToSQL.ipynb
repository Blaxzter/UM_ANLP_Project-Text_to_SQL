{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ANLP_Project_SeqToSQL.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "fb809b3677754bdcaf6aa222b5e904de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_ff93b9f5a7574239a1b225e67802a46a",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_188f24d628464342afb078d56c8a70a3",
       "IPY_MODEL_2a9a368356f44ae9bfa392a3f6b8bc4a"
      ]
     }
    },
    "ff93b9f5a7574239a1b225e67802a46a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "188f24d628464342afb078d56c8a70a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_9d6f0e8ada0e4b539bd87a078f1dbef4",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 213450,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 213450,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_4b942ac9e99f4ff9844a5c5f02f5b520"
     }
    },
    "2a9a368356f44ae9bfa392a3f6b8bc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_cc5fc7560b684aa69ad61777002378e1",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 213k/213k [00:00&lt;00:00, 536kB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_06d9784d6962497580089de9a4af5a67"
     }
    },
    "9d6f0e8ada0e4b539bd87a078f1dbef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "4b942ac9e99f4ff9844a5c5f02f5b520": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "cc5fc7560b684aa69ad61777002378e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "06d9784d6962497580089de9a4af5a67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "07e05e4e2a694c31a2c3bcdb6d5586ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_46acbe4de8594572a276b86454a11602",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_5ac7d0f4bfc64e9e912516cd9894f4ea",
       "IPY_MODEL_ed186036d82743b4bbaeb7b421bb04d8"
      ]
     }
    },
    "46acbe4de8594572a276b86454a11602": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "5ac7d0f4bfc64e9e912516cd9894f4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_13ced24dc1e2472c966d6dbe91be01ab",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 433,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 433,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_86c01d2179d74763b5b957fc942001df"
     }
    },
    "ed186036d82743b4bbaeb7b421bb04d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_fa861b8e35ee440aaef9e38342ab410b",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 433/433 [00:00&lt;00:00, 998B/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_1f43877e3c0d42da8634d6200ab588b1"
     }
    },
    "13ced24dc1e2472c966d6dbe91be01ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "86c01d2179d74763b5b957fc942001df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "fa861b8e35ee440aaef9e38342ab410b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "1f43877e3c0d42da8634d6200ab588b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "246babcbaf074305bd4f888e12fd4b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_7fded0dab11f4fdc985c1c173b7097a3",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_b52e07a6ffd443da868dd2e9225a7eeb",
       "IPY_MODEL_5f4ac871552542e1adc08228c2252256"
      ]
     }
    },
    "7fded0dab11f4fdc985c1c173b7097a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "b52e07a6ffd443da868dd2e9225a7eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_ce72d94b9a7e461fbe45ab6a6ce982da",
      "_dom_classes": [],
      "description": "Downloading: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 435779157,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 435779157,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2e1647e162c344d5914fd295d06a2914"
     }
    },
    "5f4ac871552542e1adc08228c2252256": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_1531bf8cc3ec4448ab1115b1da49f75f",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 436M/436M [00:08&lt;00:00, 53.0MB/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6ebf2f22e1f94960bf029a3a25ede4c7"
     }
    },
    "ce72d94b9a7e461fbe45ab6a6ce982da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2e1647e162c344d5914fd295d06a2914": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "1531bf8cc3ec4448ab1115b1da49f75f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "6ebf2f22e1f94960bf029a3a25ede4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73U6yPyZmhGL"
   },
   "source": [
    "# Sequence To SQL\n",
    "\n",
    "Welcome to our project in the Advanced Natural Language Processing course\n",
    "\n",
    "We try to build it with the data provided in https://github.com/salesforce/WikiSQL\n",
    "\n",
    "Remove this: https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d?gi=6b6c7e91e298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sss\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPUHkkwhVukj",
    "outputId": "d365e360-6c44-4342-f1c7-c32ebe5c8bcc"
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection and Review\n",
    "Clone the data from the WikiSQL git repository and install them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMz3n3dbHlU"
   },
   "source": [
    "Take a look inside the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zR6cyK-UbFNV"
   },
   "source": [
    "import json\n",
    "\n",
    "def read_json_data_from_file(file: str):\n",
    "  ret_data = []\n",
    "  with open(file) as json_file:\n",
    "      # Get next line from file\n",
    "      lines = json_file.readlines()\n",
    "      for line in tqdm(lines):\n",
    "          if not line:\n",
    "            break\n",
    "\n",
    "          data = json.loads(line)\n",
    "          ret_data.append(data)\n",
    "  return ret_data\n",
    "\n",
    "def convert_to_id_dict(data, id_key: str):\n",
    "  ret_dict = {}\n",
    "  for element in data:\n",
    "    if id_key in element:\n",
    "      ret_dict[element[id_key]] = element\n",
    "    else:\n",
    "      print(f'Element {element} doenst contain key {id_key}')\n",
    "  return ret_dict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A80zkDsgqjE"
   },
   "source": [
    "Lets see if we succesfully serialized the data into objects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sihMUsXg-yt",
    "outputId": "8317c04e-70bf-41bf-8f8c-c4ba39c5fd30"
   },
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "print(f'We have {len(dev_req_data)} dev data with {len(dev_table_data)} tables.')\n",
    "print(f'An example Request: ')\n",
    "print(json.dumps(dev_req_data[0], indent=2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzjw8292zYcE"
   },
   "source": [
    "### The fields represent the following:\n",
    "\n",
    "* `phase`: the phase in which the dataset was collected. We collected WikiSQL in two phases.\n",
    "* `question`: the natural language question written by the worker.\n",
    "* `table_id`: the ID of the table to which this question is addressed.\n",
    "sql: the SQL query corresponding to the question. This has the following *subfields:\n",
    "  * `sel`: the numerical index of the column that is being selected. You can find the actual column from the table.\n",
    "  * `agg`: the numerical index of the aggregation operator that is being used. You can find the actual operator from Query.agg_ops in lib/query.py.\n",
    "  * `conds`: a list of triplets (column_index, operator_index, condition) where:\n",
    "    * `column_index`: the numerical index of the condition column that is being used. You can find the actual column from the table.\n",
    "    * `operator_index`: the numerical index of the condition operator that is being used. You can find the actual operator from Query.cond_ops in lib/query.py.\n",
    "    * `condition`: the comparison value for the condition, in either string or float type.\n",
    "\n",
    "\n",
    "## An example Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f7vfdbCKGG3p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2fb59ef-25a3-4d4f-d652-5f8f0d6b796d"
   },
   "source": [
    "print(json.dumps(dev_table_data[0], indent=2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_amount = []\n",
    "for table in dev_table_data:\n",
    "    column_amount.append(len(table['header']))\n",
    "\n",
    "print(f'Amount of max columns: {np.max(column_amount)}')\n",
    "\n",
    "sns.displot(column_amount)\n",
    "plt.xlim([0, 20])\n",
    "plt.xlabel('Token count')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7i0WqeKz2Cy"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "The data is stored with indices but we need the actual column names so saturate the requests with the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gwsK8yDV2_yD"
   },
   "source": [
    "# Transform the data into a dictonary index by the id\n",
    "dev_table_data_dict = convert_to_id_dict(dev_table_data, 'id')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iIAcsIpd7OpY"
   },
   "source": [
    "# Get the preliminary data\n",
    "# Maybe we want the other idexes also"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rnvuug6C0kaK"
   },
   "source": [
    "def get_table_column(data_list, tables_dict):\n",
    "  ret_list = []\n",
    "  for element in data_list:\n",
    "    current_table = tables_dict[element['table_id']]\n",
    "    columns = current_table['header']\n",
    "    # Replace the index\n",
    "    element['columns'] = columns\n",
    "    element['types'] = current_table['types']\n",
    "    element['sql']['sel_name'] = columns[element['sql']['sel']]\n",
    "\n",
    "    if 'page_title' in current_table:\n",
    "        element['table_name'] = current_table['page_title']\n",
    "    elif 'section_title' in current_table:\n",
    "        element['table_name'] = current_table['section_title']\n",
    "    elif 'caption' in current_table:\n",
    "        element['table_name'] = current_table['caption']\n",
    "    elif 'name' in current_table:\n",
    "        element['table_name'] = current_table['name']\n",
    "\n",
    "    # For the where conditions\n",
    "    for cond in element['sql']['conds']:\n",
    "      cond[0] = columns[cond[0]]\n",
    "    ret_list.append(element)\n",
    "  return ret_list\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cv1VKePBUwf",
    "outputId": "5f52c637-26e3-47b1-c111-cd10b31d1494"
   },
   "source": [
    "dev_req_data = read_json_data_from_file(f'{data_folder}/dev.jsonl')\n",
    "dev_table_data = read_json_data_from_file(f'{data_folder}/dev.tables.jsonl')\n",
    "\n",
    "dev_prep_req_data = get_table_column(dev_req_data, dev_table_data_dict)\n",
    "\n",
    "\n",
    "print(f'Filed in with the Columns: ')\n",
    "print(json.dumps(dev_prep_req_data[-2], indent=2))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO figure out a good padding size or how to do padding correctly\n",
    "\n",
    "def get_question_answers(request, tokenizer):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    max_len = 0\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        if max_len < len(embedding['input_ids']):\n",
    "            max_len = len(embedding['input_ids'])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_question_answers_def_length(request, tokenizer, pad_max_length):\n",
    "    input_list = []\n",
    "\n",
    "    table_name = request['table_name'] #should be name not id\n",
    "    space_token = ' '\n",
    "    columns = request['columns']\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "    for i, col in enumerate(columns):\n",
    "        col_type = request['types'][i] # infere type somehow\n",
    "        column_representation = col_type + space_token + table_name + space_token + col\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            column_representation,\n",
    "            req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            truncation = True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_list.append(embedding)\n",
    "    return input_list\n",
    "\n",
    "def get_question_answers_for_where_value_def_length(request, tokenizer, pad_max_length):\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    cond_dict = {0:\"equal to \", 1:\"less than \", 2:\"more than \", 3:\"OP \"}\n",
    "\n",
    "    #agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "    #cond_ops = ['=', '>', '<', 'OP']\n",
    "\n",
    "    space_token = ' '\n",
    "    req_question = request['question'] # might need to be tokenized\n",
    "\n",
    "    conditions = request['sql']['conds']\n",
    "    for i, cond in enumerate(conditions):\n",
    "        column_name = cond[0]\n",
    "        opp_name = cond_dict[cond[1]]\n",
    "        target = cond[2]\n",
    "        value_question = column_name + space_token + opp_name\n",
    "        #print(f\"target:{target}\")\n",
    "        #print(f\"question: {req_question}\")\n",
    "        embedding = tokenizer.encode_plus(\n",
    "            text=value_question,\n",
    "            text_pair=req_question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=pad_max_length,\n",
    "            padding='max_length',\n",
    "            truncation = True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        #print(f\"embedding:{embedding}\")\n",
    "\n",
    "        input_list.append(embedding)\n",
    "        encoded_target = tokenizer.encode(text=str(target),add_special_tokens=True)[1:-1]\n",
    "        startIdx = 0\n",
    "        endIdx = 0\n",
    "        #print(f\"target:{target}\")\n",
    "        #print(f\"target_emb:{encoded_target}\")\n",
    "        sll=len(encoded_target)\n",
    "        for ind in (i for i,e in enumerate(embedding['input_ids']) if e == encoded_target[0]):\n",
    "            if embedding['input_ids'][ind:ind+sll]==encoded_target:\n",
    "                startIdx = ind\n",
    "                endIdx = ind+sll\n",
    "                break\n",
    "        #this is nessecary for some targets are uppercase while in the questions they are lowercase\n",
    "        if startIdx==0 and endIdx==0:\n",
    "            encoded_target = tokenizer.encode(text=str(target).lower(),add_special_tokens=True)[1:-1]\n",
    "            sll=len(encoded_target)\n",
    "            for ind in (i for i,e in enumerate(embedding['input_ids']) if e == encoded_target[0]):\n",
    "                if embedding['input_ids'][ind:ind+sll]==encoded_target:\n",
    "                    startIdx = ind\n",
    "                    endIdx = ind+sll\n",
    "                    break\n",
    "\n",
    "        #print(f\"start:{startIdx}, endidx: {endIdx}\")\n",
    "        target_list.append([startIdx,endIdx])\n",
    "\n",
    "    return input_list, target_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "token_amount = []\n",
    "for req in tqdm(dev_prep_req_data):\n",
    "    column_question_pairs = get_question_answers(req, tokenizer)\n",
    "    for column_question_pair in column_question_pairs:\n",
    "        token_amount.append(len(column_question_pair['input_ids']))\n",
    "\n",
    "print(f'Amount of max tockens: {np.max(token_amount)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.displot(token_amount)\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('Token count')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create data loader that transformed the stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Task(Enum):\n",
    "    SELECT = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WikiSQLDataset(Dataset):\n",
    "\n",
    "    def __init__(self, requests, tokenizer, pad_length):\n",
    "        self.requests = requests\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_length = pad_length\n",
    "\n",
    "        self.req_prepared = []\n",
    "        self.full_requests = []\n",
    "\n",
    "        self.QA_requests = []\n",
    "\n",
    "        for req in requests:\n",
    "            _qa_values, _qa_targets = get_question_answers_for_where_value_def_length(req, self.tokenizer, self.pad_length)\n",
    "            _qa_input_ids = [req_embedding['input_ids'] for req_embedding in _qa_values]\n",
    "            _qa_token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _qa_values]\n",
    "            _qa_attention_mask = [req_embedding['attention_mask'] for req_embedding in _qa_values]\n",
    "\n",
    "            _qa_where_value = torch.tensor(_qa_targets)\n",
    "\n",
    "            _req_embeddings = get_question_answers_def_length(req, self.tokenizer, self.pad_length)\n",
    "            _input_ids = [req_embedding['input_ids'] for req_embedding in _req_embeddings]\n",
    "            _token_type_ids = [req_embedding['token_type_ids'] for req_embedding in _req_embeddings]\n",
    "            _attention_mask = [req_embedding['attention_mask'] for req_embedding in _req_embeddings]\n",
    "\n",
    "            #correct_sel_id = np.zeros((len(req['columns']),1), dtype=int)\n",
    "            #correct_sel_id[req['sql']['sel']][0] = 1\n",
    "\n",
    "            correct_sel_id = np.array([req['sql']['sel']], dtype=int)\n",
    "\n",
    "            #num_agg = 6\n",
    "            #correct_agg_id =  np.zeros((num_agg,1), dtype=int)\n",
    "            #correct_agg_id[req['sql']['agg']][0] = 1\n",
    "\n",
    "            correct_agg_id = np.array([req['sql']['agg']], dtype=int)\n",
    "\n",
    "            #correct_where_conditions = []\n",
    "            # conds = [[colName, OpperationID, value],[colName, OpperationID, value]]\n",
    "\n",
    "\n",
    "            select_target = torch.tensor([correct_sel_id], dtype=torch.long)\n",
    "            select_agg_target = torch.tensor([correct_agg_id], dtype=torch.long)\n",
    "\n",
    "\n",
    "            self.req_prepared.append(dict(\n",
    "                input_ids = torch.tensor(_input_ids),\n",
    "                token_type_ids = torch.tensor(_token_type_ids),\n",
    "                attention_mask = torch.tensor(_attention_mask),\n",
    "\n",
    "                qa_input_ids = torch.tensor(_qa_input_ids),\n",
    "                qa_attention_mask = torch.tensor(_qa_attention_mask),\n",
    "                qa_token_type_ids = torch.tensor(_qa_token_type_ids),\n",
    "\n",
    "                target = dict(\n",
    "                    SELECT = select_target,\n",
    "                    SELECT_AGG = select_agg_target,\n",
    "                    WHERE_VALUE =  torch.tensor(_qa_targets)\n",
    "                )\n",
    "            ))\n",
    "            self.full_requests.append(req)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_full_request_by_id(self, req_id):\n",
    "        return self.full_requests[req_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.req_prepared)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.req_prepared[item]\n",
    "\n",
    "def get_data_loader(data_type, tokenizer, batch_size, filter_data = True, pad_length = 65):\n",
    "    # TODO check if we can use DataLoader with batch size as done in the tutorial\n",
    "    loaded_req = read_json_data_from_file(f'{data_folder}/{data_type}.jsonl')\n",
    "    loaded_tables = read_json_data_from_file(f'{data_folder}/{data_type}.tables.jsonl')\n",
    "    table_data_dict = convert_to_id_dict(loaded_tables, 'id')\n",
    "\n",
    "    prep_req_data = get_table_column(loaded_req, table_data_dict)\n",
    "\n",
    "    if filter_data:\n",
    "        prep_req_data = list(filter(lambda request: len(request['columns']) == 5, prep_req_data))\n",
    "\n",
    "    print(f'We have {len(loaded_req)} {data_type} data with {len(loaded_tables)} tables.')\n",
    "\n",
    "    return DataLoader(\n",
    "        WikiSQLDataset(requests = prep_req_data, tokenizer = tokenizer, pad_length = pad_length),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "data_folder = '../data'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for d in train_data_loader:\n",
    "    try:\n",
    "        #d = next(iterator)\n",
    "        if d[\"input_ids\"].shape[2] != 65 or d[\"input_ids\"].shape[1] != 5:\n",
    "            print(f'input_ids: {d[\"input_ids\"].shape}')\n",
    "        if d[\"attention_mask\"].shape[2] != 65 or d[\"attention_mask\"].shape[1] != 5:\n",
    "            print(f'attention_mask: {d[\"attention_mask\"].shape}')\n",
    "        if d[\"token_type_ids\"].shape[2] != 65 or d[\"token_type_ids\"].shape[1] != 5:\n",
    "            print(f'token_type_ids: {d[\"token_type_ids\"].shape}')\n",
    "        # print(f'target: {d[\"target\"][\"SELECT\"].shape}')\n",
    "    except:\n",
    "        print(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test question answer to Build the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnTnkOXH66r"
   },
   "source": [
    "In this section, we will look into **contextual embeddings**.\n",
    "\n",
    "For this we use [**pretrained BERT**](https://www.aclweb.org/anthology/N19-1423.pdf) provided via [HuggingFace](https://huggingface.co/).\n",
    "\n",
    "Let's first install the HuggingFace python package:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ndN0Emuk2K-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# reference is hydranet https://arxiv.org/pdf/2008.04759.pdf\n",
    "# Dp imports here\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Generall model configuration\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dev_prep_req_data[0])\n",
    "for tokens in get_question_answers(dev_prep_req_data[0], tokenizer):\n",
    "    print(tokens)\n",
    "    print(tokenizer.convert_ids_to_tokens(tokens['input_ids']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyLYVYNrC7Pl",
    "outputId": "827654c5-31a4-4e57-c9ef-4760e5cf20eb"
   },
   "source": [
    "req_embeddings = get_question_answers(dev_prep_req_data[-2], tokenizer)\n",
    "print(req_embeddings)\n",
    "input_ids = [req_embedding['input_ids'] for req_embedding in req_embeddings]\n",
    "token_type_ids = [req_embedding['token_type_ids'] for req_embedding in req_embeddings]\n",
    "attention_mask = [req_embedding['attention_mask'] for req_embedding in req_embeddings]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids_tensor = torch.tensor(input_ids)\n",
    "token_type_ids_tensor = torch.tensor(token_type_ids)\n",
    "attention_mask_tensor = torch.tensor(attention_mask)\n",
    "\n",
    "print(input_ids_tensor.shape)\n",
    "print(token_type_ids_tensor.shape)\n",
    "print(attention_mask_tensor.shape)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids_tensor, # The tokens representing our input text.\n",
    "    attention_mask=token_type_ids_tensor,\n",
    "    token_type_ids=attention_mask_tensor\n",
    ") # The segment IDs to differentiate question from answer_text\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'last hidden state  : {outputs.last_hidden_state.shape}')\n",
    "print(f'pooled output layer: {outputs.pooler_output.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# weight = torch.rand(model.config.hidden_size)\n",
    "# F.sigmoid(F.linear(outputs.pooler_output, weight))\n",
    "\n",
    "out = nn.Linear(model.config.hidden_size, 1)\n",
    "out2 = torch.softmax(torch.sigmoid(out(outputs.pooler_output)), dim = 0)\n",
    "\n",
    "torch.argmax(out2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BertBaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertBaseModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.squeeze(0),\n",
    "            attention_mask=attention_mask.squeeze(0),\n",
    "            token_type_ids=token_type_ids.squeeze(0)\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "class SelectionRanker(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SelectionRanker, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output)\n",
    "        softmax = torch.softmax(torch.sigmoid(linear), dim = 0)\n",
    "        return torch.transpose(softmax, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        for d in tepoch:\n",
    "            # req = d[\"request\"]\n",
    "            input_id_batch = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "            attention_mask_batch = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "            token_type_ids_batch = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "            targets_batsch = d[\"target\"]['SELECT'].to(device)\n",
    "\n",
    "            for batchIdx in range(len(input_id_batch)):\n",
    "                input_ids = input_id_batch[batchIdx]# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                attention_mask = attention_mask_batch[batchIdx] # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                token_type_ids = token_type_ids_batch[batchIdx] # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                targets = targets_batsch[batchIdx][0]\n",
    "\n",
    "                #print(f\"input_ids.shape:{input_ids.shape}\")\n",
    "                #print(f\"attention_mask.shape:{attention_mask.shape}\")\n",
    "                #print(f\"token_type_ids .shape:{token_type_ids .shape}\")\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids= token_type_ids,\n",
    "                )\n",
    "                pred_req_id = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                #print(f\"output.shape:{outputs}\")\n",
    "                #print(f\"pred_req_id.shape:{pred_req_id}\")\n",
    "                #print(f\"target.shape:{targets.shape}\")\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                correct_predictions += 1 if pred_req_id == targets else 0\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "            for d in tepoch:\n",
    "                # req = d[\"request\"]\n",
    "                input_id_batch = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                attention_mask_batch = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                token_type_ids_batch = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                targets_batsch = d[\"target\"]['SELECT'].to(device)\n",
    "\n",
    "                for batchIdx in range(len(input_id_batch)):\n",
    "                    input_ids = input_id_batch[batchIdx]# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "                    attention_mask = attention_mask_batch[batchIdx] # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "                    token_type_ids = token_type_ids_batch[batchIdx] # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "                    targets = targets_batsch[batchIdx][0]\n",
    "\n",
    "                    #print(f\"input_ids.shape:{input_ids.shape}\")\n",
    "                    #print(f\"attention_mask.shape:{attention_mask.shape}\")\n",
    "                    #print(f\"token_type_ids .shape:{token_type_ids .shape}\")\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids= token_type_ids,\n",
    "                    )\n",
    "                    pred_req_id = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    #print(f\"output.shape:{outputs}\")\n",
    "                    #print(f\"pred_req_id.shape:{pred_req_id}\")\n",
    "                    #print(f\"target.shape:{targets.shape}\")\n",
    "\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    correct_predictions += 1 if pred_req_id == targets else 0\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "        return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "bert_model = BertBaseModel()\n",
    "selection_ranker = SelectionRanker(bert_model)\n",
    "selection_ranker = selection_ranker.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(dev_prep_req_data) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "train_data_loader = get_data_loader(data_type='train', tokenizer = tokenizer, batch_size = 16)\n",
    "val_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)\n",
    "# test_data_loader = get_data_loader('train', tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"started training!\")\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    selection_ranker,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train_data_loader)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    selection_ranker,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(val_data_loader)\n",
    "  )\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data_loader = get_data_loader(data_type='dev', tokenizer = tokenizer, batch_size = 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertBaseModelBatches(nn.Module):\n",
    "    def __init__(self, config=None):\n",
    "        super(BertBaseModelBatches, self).__init__()\n",
    "        if config:\n",
    "            self.bert_model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids.flatten(end_dim=1),\n",
    "            attention_mask=attention_mask.flatten(end_dim=1),\n",
    "            token_type_ids=token_type_ids.flatten(end_dim=1)\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "class SelectionRankerBatches(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SelectionRankerBatches, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.bert.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        linear = self.linear(output.view(16, 5, 768))\n",
    "        softmax = torch.softmax(\n",
    "            torch.sigmoid(linear),\n",
    "            dim = 1\n",
    "        )\n",
    "        return softmax\n",
    "\n",
    "bert_model = BertBaseModelBatches()\n",
    "selection_ranker = SelectionRankerBatches(bert_model)\n",
    "\n",
    "d = next(iter(test_data_loader))\n",
    "input_ids = d[\"input_ids\"].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "targets = d[\"target\"][\"SELECT\"].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(token_type_ids.shape)\n",
    "\n",
    "# input_ids.flatten(end_dim=1).view(16,5,65).shape\n",
    "\n",
    "outputs = selection_ranker(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids= token_type_ids,\n",
    ")\n",
    "\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1).shape)\n",
    "print(targets.shape, targets.squeeze(1).shape)\n",
    "\n",
    "print(outputs.shape, outputs.squeeze(1))\n",
    "print(targets.shape, targets.squeeze(1))\n",
    "# I dont get why targets is shape [1, 1] and not [1] it is initilized the same way in the data loader\n",
    "\n",
    "# target = torch.tensor([2], dtype=torch.long).to(device)\n",
    "# print(target.shape, target)\n",
    "\n",
    "\n",
    "loss = loss_fn(outputs, targets.squeeze(1))\n",
    "print(loss)\n",
    "\n",
    "correct_prediction = torch.sum(torch.argmax(outputs, dim=1) == targets.squeeze(1))\n",
    "print(f'Number of correct predictions {correct_prediction}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QABert(nn.Module):\n",
    "    def __init__(self, bert_model, output_num_words):\n",
    "        super(QABert, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.linearStart = nn.Linear(self.bert.bert.config.hidden_size, output_num_words)\n",
    "        self.linearEnd= nn.Linear(self.bert.bert.config.hidden_size, output_num_words)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        startValues = self.linearStart(outputs.last_hidden_state)\n",
    "        startSoftmax = torch.softmax(startValues, dim=1)\n",
    "\n",
    "        endValues = self.linearEnd(outputs.last_hidden_state)\n",
    "        endSoftmax = torch.softmax(endValues, dim=1)\n",
    "\n",
    "        return startSoftmax, endSoftmax\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print('Current device:', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print('Failed to find GPU. Will use CPU.')\n",
    "    device = 'cpu'\n",
    "\n",
    "d = next(iter(train_data_loader))\n",
    "input_ids = d[\"input_ids\"][0].to(device)# [_input_ids.to(device) for _input_ids in d[\"input_ids\"]]\n",
    "attention_mask = d[\"attention_mask\"][0].to(device) # [_attention_mask.to(device) for _attention_mask in d[\"attention_mask\"]]\n",
    "token_type_ids = d[\"token_type_ids\"][0].to(device) # [_token_type_ids.to(device) for _token_type_ids in d[\"token_type_ids\"]]\n",
    "\n",
    "\n",
    "\n",
    "#Tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#question = '''wage less than '''\n",
    "#paragraph = '''show please me all employees with a salary smaller than 400 or where the last name is \"yannick\"'''\n",
    "\n",
    "#encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True)\n",
    "\n",
    "#inputs = torch.tensor(encoding['input_ids'])  #Token embeddings\n",
    "#sentence_embedding = torch.tensor(encoding['token_type_ids'])  #Segment embeddings\n",
    "#attention_mask = torch.tensor(encoding['attention_mask'])\n",
    "\n",
    "\n",
    "modelWV = QABert(bert_model,1)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(token_type_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "\n",
    "output = modelWV(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "start_scores = output[0]\n",
    "print(f\"start_scores: {start_scores.shape}\")\n",
    "end_scores = output[1]\n",
    "\n",
    "start_index = torch.argmax(start_scores, dim=1)\n",
    "\n",
    "print(f\"start_index{start_index}\")\n",
    "\n",
    "end_index = torch.argmax(end_scores, dim=1)\n",
    "\n",
    "answer = ' '.join(tokens[start_index:end_index+1])\n",
    "print(answer)\n",
    "corrected_answer = ''\n",
    "\n",
    "for word in answer.split():\n",
    "    #If it's a subword token\n",
    "    if word[0:2] == '##':\n",
    "        corrected_answer += word[2:]\n",
    "    else:\n",
    "        corrected_answer += ' ' + word\n",
    "print(corrected_answer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}